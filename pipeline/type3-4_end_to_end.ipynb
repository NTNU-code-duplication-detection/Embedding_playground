{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553e98cc",
   "metadata": {},
   "source": [
    "# End-to-end (pure Python) pipeline: Type-3 clone dataset → compile/decompile → AST graphs → embeddings → GNN training\n",
    "\n",
    "This notebook runs the full pipeline **without using any `python -m ...cli` commands**. It builds program artifacts from the **synthetic Code-Clone dataset** (`base/`, `type-1/2/3/`), generates method graphs from **decompiled** code, embeds nodes with **GraphCodeBERT**, and trains a small **graph encoder + program pooling + pair classifier**.\n",
    "\n",
    "Notes:\n",
    "- Uses Tree-sitter Java for parsing and a lightweight graph schema: **SEQ**, **AST**, **IF_THEN**, **IF_ELSE**.\n",
    "- Uses a small message-passing GNN implemented directly in PyTorch (no PyG dependency).\n",
    "- You can switch to training on original source instead of decompiled by toggling `USE_DECOMPILED`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e946303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterator, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_java\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d686e",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc52673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_ROOT: /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset\n",
      "OUT_DIR     : /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/pipeline/nb_program_artifacts_type3\n",
      "DEVICE      : mps\n"
     ]
    }
   ],
   "source": [
    "Pair = Tuple[str, str, int]\n",
    "CloneTypes = Union[str, Sequence[str]]\n",
    "\n",
    "# ---- Dataset ----\n",
    "DATASET_ROOT = Path(\"../data/code-clone-dataset/dataset\").expanduser().resolve()\n",
    "CLONE_TYPES: CloneTypes = [\"type-3\", \"type-4\"]       # \"type-1\" | \"type-2\" | \"type-3\"\n",
    "LIMIT_INDICES: Optional[int] = None   # set None for all\n",
    "SEED = 0\n",
    "\n",
    "# ---- Tools ----\n",
    "JDK_HOME = Path(os.environ.get(\"JAVA_HOME\", \"\")).expanduser()\n",
    "VINEFLOWER_JAR = Path(\"../chatgpt/vineflower-1.11.2.jar\").expanduser().resolve()\n",
    "\n",
    "# ---- Output ----\n",
    "OUT_DIR = Path(\"./nb_program_artifacts_type3\").resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Embedding model ----\n",
    "MODEL_NAME = \"microsoft/graphcodebert-base\"\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ---- Pipeline toggles ----\n",
    "USE_DECOMPILED = True      # If True: compile->jar->decompile and graph decompiled java\n",
    "FORCE_REBUILD = False      # If True: delete and rebuild existing program artifacts\n",
    "\n",
    "print(\"DATASET_ROOT:\", DATASET_ROOT)\n",
    "print(\"OUT_DIR     :\", OUT_DIR)\n",
    "print(\"DEVICE      :\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a941b",
   "metadata": {},
   "source": [
    "## 2) Dataset enumeration and pair generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f50eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/188/main.java -> /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/type-3/04/3.java\n",
      "1 0 /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/160/main.java -> /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/type-4/25/2.java\n",
      "2 1 /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/180/main.java -> /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/type-4/180/1.java\n",
      "3 1 /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/100/main.java -> /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/type-3/100/3.java\n",
      "4 0 /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/136/main.java -> /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/type-3/175/2.java\n"
     ]
    }
   ],
   "source": [
    "def _normalize_clone_types(clone_types: CloneTypes) -> List[str]:\n",
    "    if isinstance(clone_types, str):\n",
    "        return [clone_types]\n",
    "    return list(clone_types)\n",
    "\n",
    "\n",
    "def list_indices(root: Path) -> List[str]:\n",
    "    base = root / \"base\"\n",
    "    return [p.name for p in sorted(base.iterdir()) if p.is_dir()]\n",
    "\n",
    "\n",
    "def anchor_path(root: Path, idx: str) -> Path:\n",
    "    return root / \"base\" / idx / \"main.java\"\n",
    "\n",
    "\n",
    "def clone_paths(root: Path, idx: str, clone_type: str) -> List[Path]:\n",
    "    d = root / clone_type / idx\n",
    "    if not d.exists():\n",
    "        return []\n",
    "    return sorted(d.glob(\"*.java\"))\n",
    "\n",
    "\n",
    "def positive_pairs(\n",
    "    *,\n",
    "    root: Path,\n",
    "    clone_types: CloneTypes,\n",
    "    seed: int = 0,\n",
    "    limit_indices: Optional[int] = None,\n",
    "    infinite: bool = True,\n",
    ") -> Iterator[Pair]:\n",
    "    \"\"\"\n",
    "    Positive: (base/<idx>/main.java) -> (type-k/<idx>/*.java), label=1\n",
    "    If multiple clone_types are given, we sample among available types for that idx.\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    types = _normalize_clone_types(clone_types)\n",
    "\n",
    "    idxs = list_indices(root)\n",
    "    if limit_indices is not None:\n",
    "        idxs = idxs[:limit_indices]\n",
    "\n",
    "    per: List[Tuple[str, Path, List[Path]]] = []\n",
    "    for idx in idxs:\n",
    "        a = anchor_path(root, idx)\n",
    "        if not a.exists():\n",
    "            continue\n",
    "\n",
    "        cs_all: List[Path] = []\n",
    "        for t in types:\n",
    "            cs_all.extend(clone_paths(root, idx, t))\n",
    "\n",
    "        if cs_all:\n",
    "            per.append((idx, a, cs_all))\n",
    "\n",
    "    if not per:\n",
    "        raise RuntimeError(f\"No positive pairs found. Check DATASET_ROOT and clone_types={types}.\")\n",
    "\n",
    "    while True:\n",
    "        _, a, cs_all = rng.choice(per)\n",
    "        b = rng.choice(cs_all)\n",
    "        yield (str(a), str(b), 1)\n",
    "        if not infinite:\n",
    "            return\n",
    "\n",
    "\n",
    "def negative_pairs(\n",
    "    *,\n",
    "    root: Path,\n",
    "    clone_types: CloneTypes,\n",
    "    seed: int = 0,\n",
    "    limit_indices: Optional[int] = None,\n",
    "    infinite: bool = True,\n",
    ") -> Iterator[Pair]:\n",
    "    \"\"\"\n",
    "    Negative: (base/<idxA>/main.java) -> (type-k/<idxB>/*.java), idxB != idxA, label=0\n",
    "    Pool includes clones from ALL configured clone_types.\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed + 12345)\n",
    "    types = _normalize_clone_types(clone_types)\n",
    "\n",
    "    idxs = list_indices(root)\n",
    "    if limit_indices is not None:\n",
    "        idxs = idxs[:limit_indices]\n",
    "\n",
    "    anchors = [(idx, anchor_path(root, idx)) for idx in idxs]\n",
    "    anchors = [(idx, p) for idx, p in anchors if p.exists()]\n",
    "\n",
    "    clone_pool: List[Tuple[str, Path]] = []\n",
    "    for idx in idxs:\n",
    "        for t in types:\n",
    "            for p in clone_paths(root, idx, t):\n",
    "                if p.exists():\n",
    "                    clone_pool.append((idx, p))\n",
    "\n",
    "    if not anchors or not clone_pool:\n",
    "        raise RuntimeError(f\"No negative pools found. clone_types={types}\")\n",
    "\n",
    "    while True:\n",
    "        idx_a, a = rng.choice(anchors)\n",
    "        while True:\n",
    "            idx_b, b = rng.choice(clone_pool)\n",
    "            if idx_b != idx_a:\n",
    "                break\n",
    "        yield (str(a), str(b), 0)\n",
    "        if not infinite:\n",
    "            return\n",
    "\n",
    "\n",
    "def interleave(pos_it, neg_it, pos_ratio: float = 0.5, seed: int = 0) -> Iterator[Pair]:\n",
    "    rng = random.Random(seed)\n",
    "    while True:\n",
    "        yield next(pos_it) if rng.random() < pos_ratio else next(neg_it)\n",
    "\n",
    "\n",
    "# Preview\n",
    "pos = positive_pairs(root=DATASET_ROOT, clone_types=CLONE_TYPES, seed=SEED, limit_indices=LIMIT_INDICES)\n",
    "neg = negative_pairs(root=DATASET_ROOT, clone_types=CLONE_TYPES, seed=SEED, limit_indices=LIMIT_INDICES)\n",
    "mix = interleave(pos, neg, pos_ratio=0.5, seed=SEED)\n",
    "\n",
    "for i in range(5):\n",
    "    a, b, y = next(mix)\n",
    "    print(i, y, a, \"->\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc71b8",
   "metadata": {},
   "source": [
    "## 3) Java compile + jar + decompile helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b82aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLASS_RE = re.compile(r\"public\\s+class\\s+([A-Za-z_]\\w*)\")\n",
    "\n",
    "def detect_public_class_name(java_text: str) -> Optional[str]:\n",
    "    m = CLASS_RE.search(java_text)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def run(cmd: List[str], cwd: Optional[Path] = None, timeout: int = 300) -> Tuple[int,str,str]:\n",
    "    p = subprocess.run(\n",
    "        cmd,\n",
    "        cwd=str(cwd) if cwd else None,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    return p.returncode, p.stdout, p.stderr\n",
    "\n",
    "def compile_single_java(java_path: Path, work_dir: Path, jdk_home: Path, extra_javac: Optional[List[str]] = None) -> Path:\n",
    "    work_dir.mkdir(parents=True, exist_ok=True)\n",
    "    src_dir = work_dir / \"src\"\n",
    "    cls_dir = work_dir / \"classes\"\n",
    "    src_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cls_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    text = java_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    cls = detect_public_class_name(text) or java_path.stem\n",
    "    target_java = src_dir / f\"{cls}.java\"\n",
    "    target_java.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "    javac = (jdk_home / \"bin\" / \"javac\") if jdk_home and (jdk_home / \"bin\" / \"javac\").exists() else Path(\"javac\")\n",
    "    flags = [\"-g\", \"-encoding\", \"UTF-8\", \"-d\", str(cls_dir)]\n",
    "    if extra_javac:\n",
    "        flags.extend(extra_javac)\n",
    "\n",
    "    rc, out, err = run([str(javac), *flags, str(target_java)], cwd=work_dir, timeout=300)\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"javac failed\\n{err}\\n\")\n",
    "\n",
    "    jar = (jdk_home / \"bin\" / \"jar\") if jdk_home and (jdk_home / \"bin\" / \"jar\").exists() else Path(\"jar\")\n",
    "    jar_path = work_dir / \"app.jar\"\n",
    "    rc, out, err = run([str(jar), \"--create\", \"--file\", str(jar_path), \"-C\", str(cls_dir), \".\"], cwd=work_dir, timeout=300)\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"jar failed\\n{err}\\n\")\n",
    "    return jar_path\n",
    "\n",
    "def decompile_jar(jar_path: Path, out_dir: Path, vineflower_jar: Path) -> Path:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    rc, out, err = run([\"java\", \"-jar\", str(vineflower_jar), str(jar_path), str(out_dir)], cwd=out_dir, timeout=300)\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"vineflower failed\\n{err}\\n\")\n",
    "    return out_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c025a5",
   "metadata": {},
   "source": [
    "## 4) Tree-sitter Java parser + lightweight AST graph builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f34ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes: 5 edges: 12 edge_types: ['AST', 'IF_ELSE', 'IF_THEN', 'SEQ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "JAVA_LANGUAGE = Language(tree_sitter_java.language())\n",
    "parser = Parser(JAVA_LANGUAGE)\n",
    "\n",
    "CONTROL_NODES = {\n",
    "    \"if_statement\",\"for_statement\",\"while_statement\",\"do_statement\",\"switch_statement\",\n",
    "    \"try_statement\",\"catch_clause\",\n",
    "}\n",
    "STRAIGHT_NODES = {\n",
    "    \"local_variable_declaration\",\"expression_statement\",\"return_statement\",\"throw_statement\",\n",
    "}\n",
    "\n",
    "EDGE_TYPE_TO_ID = {\"SEQ\":0, \"AST\":1, \"IF_THEN\":2, \"IF_ELSE\":3}\n",
    "\n",
    "def _safe_text(code_bytes: bytes, node) -> str:\n",
    "    return code_bytes[node.start_byte:node.end_byte].decode(\"utf-8\", errors=\"replace\").strip()\n",
    "\n",
    "def extract_control_header(code: bytes, node) -> str:\n",
    "    for child in node.children:\n",
    "        if child.type in (\"condition\", \"parenthesized_expression\"):\n",
    "            return code[child.start_byte:child.end_byte].decode(\"utf-8\", errors=\"replace\").strip()\n",
    "    text = code[node.start_byte:node.end_byte].decode(\"utf-8\", errors=\"replace\")\n",
    "    return text.split(\"{\")[0].strip()\n",
    "\n",
    "@dataclass\n",
    "class GraphNode:\n",
    "    id: int\n",
    "    kind: str               # \"control\" | \"straight\"\n",
    "    ast_type: str\n",
    "    code: str\n",
    "    start_byte: int\n",
    "    end_byte: int\n",
    "    depth: int\n",
    "\n",
    "@dataclass\n",
    "class GraphEdge:\n",
    "    src: int\n",
    "    dst: int\n",
    "    type: str               # SEQ | AST | IF_THEN | IF_ELSE\n",
    "\n",
    "def build_method_graph(java_source: str) -> Dict[str, Any]:\n",
    "    code_bytes = java_source.encode(\"utf-8\")\n",
    "    tree = parser.parse(code_bytes)\n",
    "\n",
    "    nodes: List[GraphNode] = []\n",
    "    edges: List[GraphEdge] = []\n",
    "    next_id = 0\n",
    "    span_to_gid: Dict[Tuple[int,int,str], int] = {}\n",
    "\n",
    "    def add_node(ts_node, depth: int) -> Optional[int]:\n",
    "        nonlocal next_id\n",
    "        t = ts_node.type\n",
    "        is_control = t in CONTROL_NODES\n",
    "        is_straight = t in STRAIGHT_NODES\n",
    "        if not (is_control or is_straight):\n",
    "            return None\n",
    "        if is_control:\n",
    "            code = extract_control_header(code_bytes, ts_node)\n",
    "            kind = \"control\"\n",
    "        else:\n",
    "            code = _safe_text(code_bytes, ts_node)\n",
    "            kind = \"straight\"\n",
    "        if not code:\n",
    "            return None\n",
    "        gid = next_id; next_id += 1\n",
    "        nodes.append(GraphNode(\n",
    "            id=gid, kind=kind, ast_type=t, code=code,\n",
    "            start_byte=ts_node.start_byte, end_byte=ts_node.end_byte, depth=depth\n",
    "        ))\n",
    "        span_to_gid[(ts_node.start_byte, ts_node.end_byte, ts_node.type)] = gid\n",
    "        return gid\n",
    "\n",
    "    stack: List[Tuple[Any,int,Optional[int]]] = [(tree.root_node, 0, None)]\n",
    "    while stack:\n",
    "        ts_node, depth, parent_sel = stack.pop()\n",
    "        cur = add_node(ts_node, depth)\n",
    "        next_parent = parent_sel\n",
    "        if cur is not None:\n",
    "            if parent_sel is not None:\n",
    "                edges.append(GraphEdge(parent_sel, cur, \"AST\"))\n",
    "                edges.append(GraphEdge(cur, parent_sel, \"AST\"))\n",
    "            next_parent = cur\n",
    "        for ch in reversed(ts_node.children):\n",
    "            if len(ch.children) == 0 and ch.type in (\";\", \"{\", \"}\", \"(\", \")\", \",\"):\n",
    "                continue\n",
    "            stack.append((ch, depth+1, next_parent))\n",
    "\n",
    "    if len(nodes) >= 2:\n",
    "        top_depth = min(n.depth for n in nodes)\n",
    "        top_nodes = [n for n in nodes if n.depth == top_depth]\n",
    "        top_nodes.sort(key=lambda n: (n.start_byte, n.end_byte))\n",
    "        for a,b in zip(top_nodes, top_nodes[1:]):\n",
    "            edges.append(GraphEdge(a.id, b.id, \"SEQ\"))\n",
    "            edges.append(GraphEdge(b.id, a.id, \"SEQ\"))\n",
    "\n",
    "    stack = [tree.root_node]\n",
    "    while stack:\n",
    "        ts_node = stack.pop()\n",
    "        if ts_node.type == \"if_statement\":\n",
    "            if_id = span_to_gid.get((ts_node.start_byte, ts_node.end_byte, ts_node.type))\n",
    "            if if_id is not None:\n",
    "                then_node = ts_node.child_by_field_name(\"consequence\")\n",
    "                else_node = ts_node.child_by_field_name(\"alternative\")\n",
    "\n",
    "                def selected_in_span(s: int, e: int) -> List[int]:\n",
    "                    out = []\n",
    "                    for n in nodes:\n",
    "                        if n.start_byte >= s and n.end_byte <= e:\n",
    "                            out.append(n.id)\n",
    "                    return out\n",
    "\n",
    "                if then_node is not None:\n",
    "                    for tid in selected_in_span(then_node.start_byte, then_node.end_byte):\n",
    "                        if tid == if_id: \n",
    "                            continue\n",
    "                        edges.append(GraphEdge(if_id, tid, \"IF_THEN\"))\n",
    "                        edges.append(GraphEdge(tid, if_id, \"IF_THEN\"))\n",
    "\n",
    "                if else_node is not None:\n",
    "                    for eid in selected_in_span(else_node.start_byte, else_node.end_byte):\n",
    "                        if eid == if_id:\n",
    "                            continue\n",
    "                        edges.append(GraphEdge(if_id, eid, \"IF_ELSE\"))\n",
    "                        edges.append(GraphEdge(eid, if_id, \"IF_ELSE\"))\n",
    "\n",
    "        for ch in reversed(ts_node.children):\n",
    "            if len(ch.children) == 0 and ch.type in (\";\", \"{\", \"}\", \"(\", \")\", \",\"):\n",
    "                continue\n",
    "            stack.append(ch)\n",
    "\n",
    "    return {\n",
    "        \"nodes\": [asdict(n) for n in nodes],\n",
    "        \"edges\": [asdict(e) for e in edges],\n",
    "    }\n",
    "\n",
    "def edges_to_tensors(edges: List[Dict[str,Any]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    src, dst, et = [], [], []\n",
    "    for e in edges:\n",
    "        t = e[\"type\"]\n",
    "        if t not in EDGE_TYPE_TO_ID:\n",
    "            continue\n",
    "        src.append(int(e[\"src\"]))\n",
    "        dst.append(int(e[\"dst\"]))\n",
    "        et.append(int(EDGE_TYPE_TO_ID[t]))\n",
    "    if not src:\n",
    "        return torch.zeros((2,0), dtype=torch.long), torch.zeros((0,), dtype=torch.long)\n",
    "    return torch.tensor([src,dst], dtype=torch.long), torch.tensor(et, dtype=torch.long)\n",
    "\n",
    "# Sanity test\n",
    "demo = \"class Demo{ int foo(int a,int b){ int x=a; if(a>b){x=x+1;} else {x=x-1;} return x; } }\"\n",
    "g = build_method_graph(demo)\n",
    "ei, et = edges_to_tensors(g[\"edges\"])\n",
    "print(\"nodes:\", len(g[\"nodes\"]), \"edges:\", ei.shape[1], \"edge_types:\", sorted({e['type'] for e in g['edges']}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f67d7",
   "metadata": {},
   "source": [
    "## 5) Method extraction from a Java file (Tree-sitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25adb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_methods(java_text: str) -> List[Tuple[str, str]]:\n",
    "    code_bytes = java_text.encode(\"utf-8\")\n",
    "    tree = parser.parse(code_bytes)\n",
    "    root = tree.root_node\n",
    "\n",
    "    out: List[Tuple[str,str]] = []\n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        n = stack.pop()\n",
    "        if n.type in (\"method_declaration\", \"constructor_declaration\"):\n",
    "            name = None\n",
    "            for ch in n.children:\n",
    "                if ch.type == \"identifier\":\n",
    "                    name = code_bytes[ch.start_byte:ch.end_byte].decode(\"utf-8\", errors=\"replace\")\n",
    "                    break\n",
    "            if name is None:\n",
    "                name = n.type\n",
    "            src = code_bytes[n.start_byte:n.end_byte].decode(\"utf-8\", errors=\"replace\")\n",
    "            out.append((name, src))\n",
    "        for ch in reversed(n.children):\n",
    "            stack.append(ch)\n",
    "    return out\n",
    "\n",
    "def java_file_to_methods_jsonl(java_path: Path, out_jsonl: Path) -> int:\n",
    "    text = java_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    methods = extract_methods(text)\n",
    "    if not methods:\n",
    "        methods = [(\"FILE\", text)]\n",
    "    out_jsonl.parent.mkdir(parents=True, exist_ok=True)\n",
    "    n_written = 0\n",
    "    with out_jsonl.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i,(mname, msrc) in enumerate(methods):\n",
    "            g = build_method_graph(msrc)\n",
    "            rec = {\n",
    "                \"method_id\": f\"{java_path}:{mname}:{i}\",\n",
    "                \"method_name\": mname,\n",
    "                \"file\": str(java_path),\n",
    "                \"nodes\": [\n",
    "                    {\n",
    "                        \"id\": n[\"id\"],\n",
    "                        \"kind\": n[\"kind\"],\n",
    "                        \"ast_type\": n[\"ast_type\"],\n",
    "                        \"code\": n[\"code\"],\n",
    "                        \"start_byte\": n[\"start_byte\"],\n",
    "                        \"end_byte\": n[\"end_byte\"],\n",
    "                        \"depth\": n[\"depth\"],\n",
    "                    }\n",
    "                    for n in g[\"nodes\"]\n",
    "                ],\n",
    "                \"edges\": g[\"edges\"],\n",
    "            }\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            n_written += 1\n",
    "    return n_written\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8e19f",
   "metadata": {},
   "source": [
    "## 6) Build per-program artifacts (compile/decompile → methods.jsonl → embed_shard.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ab74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pick_java_for_graph(program_src: Path, artifact_dir: Path) -> Path:\n",
    "    if not USE_DECOMPILED:\n",
    "        return program_src\n",
    "    decomp_dir = artifact_dir / \"decompiled\"\n",
    "    cands = sorted(decomp_dir.rglob(\"*.java\"))\n",
    "    return cands[0] if cands else program_src\n",
    "\n",
    "def embed_methods_jsonl(in_jsonl: Path, out_pt: Path, model, tokenizer, device: str,\n",
    "                        max_length: int = 256, batch_size: int = 32) -> int:\n",
    "    recs = []\n",
    "    parsed = [json.loads(l) for l in in_jsonl.read_text(encoding=\"utf-8\").splitlines() if l.strip()]\n",
    "\n",
    "    for rec in parsed:\n",
    "        nodes = rec[\"nodes\"]\n",
    "        edges = rec[\"edges\"]\n",
    "        texts = [n[\"code\"] for n in nodes]\n",
    "\n",
    "        if not texts:\n",
    "            edge_index, edge_type = edges_to_tensors(edges)\n",
    "            recs.append({\n",
    "                \"method_id\": rec[\"method_id\"],\n",
    "                \"method_name\": rec[\"method_name\"],\n",
    "                \"file\": rec[\"file\"],\n",
    "                \"x\": torch.zeros((0, 768), dtype=torch.float32),\n",
    "                \"edge_index\": edge_index,\n",
    "                \"edge_type\": edge_type,\n",
    "                \"num_nodes\": 0,\n",
    "                \"num_edges\": int(edge_index.shape[1]),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        embs = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            tok = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "            tok = {k:v.to(device) for k,v in tok.items()}\n",
    "            with torch.no_grad():\n",
    "                out = model(**tok)\n",
    "                last = out.last_hidden_state\n",
    "                mask = tok[\"attention_mask\"].unsqueeze(-1)\n",
    "                pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "            embs.append(pooled.detach().cpu())\n",
    "        x = torch.cat(embs, dim=0).to(torch.float32)\n",
    "\n",
    "        edge_index, edge_type = edges_to_tensors(edges)\n",
    "        recs.append({\n",
    "            \"method_id\": rec[\"method_id\"],\n",
    "            \"method_name\": rec[\"method_name\"],\n",
    "            \"file\": rec[\"file\"],\n",
    "            \"x\": x,\n",
    "            \"edge_index\": edge_index,\n",
    "            \"edge_type\": edge_type,\n",
    "            \"num_nodes\": int(x.shape[0]),\n",
    "            \"num_edges\": int(edge_index.shape[1]),\n",
    "        })\n",
    "\n",
    "    out_pt.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(recs, out_pt)\n",
    "    return len(recs)\n",
    "\n",
    "def build_program_artifact(program_src: Path, out_root: Path,\n",
    "                           jdk_home: Path, vineflower_jar: Path,\n",
    "                           model, tokenizer, device: str,\n",
    "                           extra_javac: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    prog_id = sha1(str(program_src))\n",
    "    artifact_dir = out_root / f\"prog_{prog_id}\"\n",
    "\n",
    "    if FORCE_REBUILD and artifact_dir.exists():\n",
    "        shutil.rmtree(artifact_dir)\n",
    "    artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if USE_DECOMPILED:\n",
    "        jar_path = artifact_dir / \"app.jar\"\n",
    "        if not jar_path.exists():\n",
    "            jar_path = compile_single_java(program_src, artifact_dir, jdk_home, extra_javac=extra_javac)\n",
    "        decomp_dir = artifact_dir / \"decompiled\"\n",
    "        if not decomp_dir.exists() or not any(decomp_dir.rglob(\"*.java\")):\n",
    "            decompile_jar(jar_path, decomp_dir, vineflower_jar)\n",
    "\n",
    "    java_for_graph = pick_java_for_graph(program_src, artifact_dir)\n",
    "\n",
    "    methods_jsonl = artifact_dir / \"methods.jsonl\"\n",
    "    if not methods_jsonl.exists():\n",
    "        n_methods = java_file_to_methods_jsonl(java_for_graph, methods_jsonl)\n",
    "    else:\n",
    "        n_methods = sum(1 for _ in methods_jsonl.open(\"r\", encoding=\"utf-8\"))\n",
    "\n",
    "    shard_pt = artifact_dir / \"embed_shard.pt\"\n",
    "    if not shard_pt.exists():\n",
    "        n_rec = embed_methods_jsonl(methods_jsonl, shard_pt, model, tokenizer, device=device,\n",
    "                                    max_length=MAX_LENGTH, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        n_rec = len(torch.load(shard_pt, map_location=\"cpu\"))\n",
    "\n",
    "    return {\n",
    "        \"source_path\": str(program_src),\n",
    "        \"artifact_dir\": str(artifact_dir),\n",
    "        \"java_for_graph\": str(java_for_graph),\n",
    "        \"methods_jsonl\": str(methods_jsonl),\n",
    "        \"embed_shard\": str(shard_pt),\n",
    "        \"num_methods\": int(n_methods),\n",
    "        \"num_method_records\": int(n_rec),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705ec94",
   "metadata": {},
   "source": [
    "## 7) Run artifact build for anchors + clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8617aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1732.05it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "RobertaModel LOAD REPORT from: microsoft/graphcodebert-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.decoder.bias            | UNEXPECTED | \n",
      "lm_head.decoder.weight          | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programs to process: 1400\n",
      "[5/1400] ok=5 fail=0\n",
      "[10/1400] ok=10 fail=0\n",
      "[15/1400] ok=15 fail=0\n",
      "[20/1400] ok=20 fail=0\n",
      "[25/1400] ok=25 fail=0\n",
      "[30/1400] ok=30 fail=0\n",
      "[35/1400] ok=35 fail=0\n",
      "[40/1400] ok=40 fail=0\n",
      "[45/1400] ok=45 fail=0\n",
      "[50/1400] ok=50 fail=0\n",
      "[55/1400] ok=55 fail=0\n",
      "[60/1400] ok=60 fail=0\n",
      "[65/1400] ok=65 fail=0\n",
      "[70/1400] ok=70 fail=0\n",
      "[75/1400] ok=75 fail=0\n",
      "[80/1400] ok=80 fail=0\n",
      "[85/1400] ok=85 fail=0\n",
      "[90/1400] ok=90 fail=0\n",
      "[95/1400] ok=95 fail=0\n",
      "[100/1400] ok=100 fail=0\n",
      "[105/1400] ok=105 fail=0\n",
      "[110/1400] ok=110 fail=0\n",
      "[115/1400] ok=115 fail=0\n",
      "[120/1400] ok=120 fail=0\n",
      "[125/1400] ok=125 fail=0\n",
      "[130/1400] ok=130 fail=0\n",
      "[135/1400] ok=135 fail=0\n",
      "[140/1400] ok=140 fail=0\n",
      "[145/1400] ok=145 fail=0\n",
      "[150/1400] ok=150 fail=0\n",
      "[155/1400] ok=155 fail=0\n",
      "[160/1400] ok=160 fail=0\n",
      "[165/1400] ok=165 fail=0\n",
      "[170/1400] ok=170 fail=0\n",
      "[175/1400] ok=175 fail=0\n",
      "[180/1400] ok=180 fail=0\n",
      "[185/1400] ok=185 fail=0\n",
      "[190/1400] ok=190 fail=0\n",
      "[195/1400] ok=195 fail=0\n",
      "[200/1400] ok=200 fail=0\n",
      "[205/1400] ok=205 fail=0\n",
      "[210/1400] ok=210 fail=0\n",
      "[215/1400] ok=215 fail=0\n",
      "[220/1400] ok=220 fail=0\n",
      "[225/1400] ok=225 fail=0\n",
      "[230/1400] ok=230 fail=0\n",
      "[235/1400] ok=235 fail=0\n",
      "[240/1400] ok=240 fail=0\n",
      "[245/1400] ok=245 fail=0\n",
      "[250/1400] ok=250 fail=0\n",
      "[255/1400] ok=255 fail=0\n",
      "[260/1400] ok=260 fail=0\n",
      "[265/1400] ok=265 fail=0\n",
      "[270/1400] ok=270 fail=0\n",
      "[275/1400] ok=275 fail=0\n",
      "[280/1400] ok=280 fail=0\n",
      "[285/1400] ok=285 fail=0\n",
      "[290/1400] ok=290 fail=0\n",
      "[295/1400] ok=295 fail=0\n",
      "[300/1400] ok=300 fail=0\n",
      "[305/1400] ok=305 fail=0\n",
      "[310/1400] ok=310 fail=0\n",
      "[315/1400] ok=315 fail=0\n",
      "[320/1400] ok=320 fail=0\n",
      "[325/1400] ok=325 fail=0\n",
      "[330/1400] ok=330 fail=0\n",
      "[335/1400] ok=335 fail=0\n",
      "[340/1400] ok=340 fail=0\n",
      "[345/1400] ok=345 fail=0\n",
      "[350/1400] ok=350 fail=0\n",
      "[355/1400] ok=355 fail=0\n",
      "[360/1400] ok=360 fail=0\n",
      "[365/1400] ok=365 fail=0\n",
      "[370/1400] ok=370 fail=0\n",
      "[375/1400] ok=375 fail=0\n",
      "[380/1400] ok=380 fail=0\n",
      "[385/1400] ok=385 fail=0\n",
      "[390/1400] ok=390 fail=0\n",
      "[395/1400] ok=395 fail=0\n",
      "[400/1400] ok=400 fail=0\n",
      "[405/1400] ok=405 fail=0\n",
      "[410/1400] ok=410 fail=0\n",
      "[415/1400] ok=415 fail=0\n",
      "[420/1400] ok=420 fail=0\n",
      "[425/1400] ok=425 fail=0\n",
      "[430/1400] ok=430 fail=0\n",
      "[435/1400] ok=435 fail=0\n",
      "[440/1400] ok=440 fail=0\n",
      "[445/1400] ok=445 fail=0\n",
      "[450/1400] ok=450 fail=0\n",
      "[455/1400] ok=455 fail=0\n",
      "[460/1400] ok=460 fail=0\n",
      "[465/1400] ok=465 fail=0\n",
      "[470/1400] ok=470 fail=0\n",
      "[475/1400] ok=475 fail=0\n",
      "[480/1400] ok=480 fail=0\n",
      "[485/1400] ok=485 fail=0\n",
      "[490/1400] ok=490 fail=0\n",
      "[495/1400] ok=495 fail=0\n",
      "[500/1400] ok=500 fail=0\n",
      "[505/1400] ok=505 fail=0\n",
      "[510/1400] ok=510 fail=0\n",
      "[515/1400] ok=515 fail=0\n",
      "[520/1400] ok=520 fail=0\n",
      "[525/1400] ok=525 fail=0\n",
      "[530/1400] ok=530 fail=0\n",
      "[535/1400] ok=535 fail=0\n",
      "[540/1400] ok=540 fail=0\n",
      "[545/1400] ok=545 fail=0\n",
      "[550/1400] ok=550 fail=0\n",
      "[555/1400] ok=555 fail=0\n",
      "[560/1400] ok=560 fail=0\n",
      "[565/1400] ok=565 fail=0\n",
      "[570/1400] ok=570 fail=0\n",
      "[575/1400] ok=575 fail=0\n",
      "[580/1400] ok=580 fail=0\n",
      "[585/1400] ok=585 fail=0\n",
      "[590/1400] ok=590 fail=0\n",
      "[595/1400] ok=595 fail=0\n",
      "[600/1400] ok=600 fail=0\n",
      "[605/1400] ok=605 fail=0\n",
      "[610/1400] ok=610 fail=0\n",
      "[615/1400] ok=615 fail=0\n",
      "[620/1400] ok=620 fail=0\n",
      "[625/1400] ok=625 fail=0\n",
      "[630/1400] ok=630 fail=0\n",
      "[635/1400] ok=635 fail=0\n",
      "[640/1400] ok=640 fail=0\n",
      "[645/1400] ok=645 fail=0\n",
      "[650/1400] ok=650 fail=0\n",
      "[655/1400] ok=655 fail=0\n",
      "[660/1400] ok=660 fail=0\n",
      "[665/1400] ok=665 fail=0\n",
      "[670/1400] ok=670 fail=0\n",
      "[675/1400] ok=675 fail=0\n",
      "[680/1400] ok=680 fail=0\n",
      "[685/1400] ok=685 fail=0\n",
      "[690/1400] ok=690 fail=0\n",
      "[695/1400] ok=695 fail=0\n",
      "[700/1400] ok=700 fail=0\n",
      "[705/1400] ok=705 fail=0\n",
      "[710/1400] ok=710 fail=0\n",
      "[715/1400] ok=715 fail=0\n",
      "[720/1400] ok=720 fail=0\n",
      "[725/1400] ok=725 fail=0\n",
      "[730/1400] ok=730 fail=0\n",
      "[735/1400] ok=735 fail=0\n",
      "[740/1400] ok=740 fail=0\n",
      "[745/1400] ok=745 fail=0\n",
      "[750/1400] ok=750 fail=0\n",
      "[755/1400] ok=755 fail=0\n",
      "[760/1400] ok=760 fail=0\n",
      "[765/1400] ok=765 fail=0\n",
      "[770/1400] ok=770 fail=0\n",
      "[775/1400] ok=775 fail=0\n",
      "[780/1400] ok=780 fail=0\n",
      "[785/1400] ok=785 fail=0\n",
      "[790/1400] ok=790 fail=0\n",
      "[795/1400] ok=795 fail=0\n",
      "[800/1400] ok=800 fail=0\n",
      "[805/1400] ok=805 fail=0\n",
      "[810/1400] ok=810 fail=0\n",
      "[815/1400] ok=815 fail=0\n",
      "[820/1400] ok=820 fail=0\n",
      "[825/1400] ok=825 fail=0\n",
      "[830/1400] ok=830 fail=0\n",
      "[835/1400] ok=835 fail=0\n",
      "[840/1400] ok=840 fail=0\n",
      "[845/1400] ok=845 fail=0\n",
      "[850/1400] ok=850 fail=0\n",
      "[855/1400] ok=855 fail=0\n",
      "[860/1400] ok=860 fail=0\n",
      "[865/1400] ok=865 fail=0\n",
      "[870/1400] ok=870 fail=0\n",
      "[875/1400] ok=875 fail=0\n",
      "[880/1400] ok=880 fail=0\n",
      "[885/1400] ok=885 fail=0\n",
      "[890/1400] ok=890 fail=0\n",
      "[895/1400] ok=895 fail=0\n",
      "[900/1400] ok=900 fail=0\n",
      "[905/1400] ok=905 fail=0\n",
      "[910/1400] ok=910 fail=0\n",
      "[915/1400] ok=915 fail=0\n",
      "[920/1400] ok=920 fail=0\n",
      "[925/1400] ok=925 fail=0\n",
      "[930/1400] ok=930 fail=0\n",
      "[935/1400] ok=935 fail=0\n",
      "[940/1400] ok=940 fail=0\n",
      "[945/1400] ok=945 fail=0\n",
      "[950/1400] ok=950 fail=0\n",
      "[955/1400] ok=955 fail=0\n",
      "[960/1400] ok=960 fail=0\n",
      "[965/1400] ok=965 fail=0\n",
      "[970/1400] ok=970 fail=0\n",
      "[975/1400] ok=975 fail=0\n",
      "[980/1400] ok=980 fail=0\n",
      "[985/1400] ok=985 fail=0\n",
      "[990/1400] ok=990 fail=0\n",
      "[995/1400] ok=995 fail=0\n",
      "[1000/1400] ok=1000 fail=0\n",
      "[1005/1400] ok=1005 fail=0\n",
      "[1010/1400] ok=1010 fail=0\n",
      "[1015/1400] ok=1015 fail=0\n",
      "[1020/1400] ok=1020 fail=0\n",
      "[1025/1400] ok=1025 fail=0\n",
      "[1030/1400] ok=1030 fail=0\n",
      "[1035/1400] ok=1035 fail=0\n",
      "[1040/1400] ok=1040 fail=0\n",
      "[1045/1400] ok=1045 fail=0\n",
      "[1050/1400] ok=1050 fail=0\n",
      "[1055/1400] ok=1055 fail=0\n",
      "[1060/1400] ok=1060 fail=0\n",
      "[1065/1400] ok=1065 fail=0\n",
      "[1070/1400] ok=1070 fail=0\n",
      "[1075/1400] ok=1075 fail=0\n",
      "[1080/1400] ok=1080 fail=0\n",
      "[1085/1400] ok=1085 fail=0\n",
      "[1090/1400] ok=1090 fail=0\n",
      "[1095/1400] ok=1095 fail=0\n",
      "[1100/1400] ok=1100 fail=0\n",
      "[1105/1400] ok=1105 fail=0\n",
      "[1110/1400] ok=1110 fail=0\n",
      "[1115/1400] ok=1115 fail=0\n",
      "[1120/1400] ok=1120 fail=0\n",
      "[1125/1400] ok=1125 fail=0\n",
      "[1130/1400] ok=1130 fail=0\n",
      "[1135/1400] ok=1135 fail=0\n",
      "[1140/1400] ok=1140 fail=0\n",
      "[1145/1400] ok=1145 fail=0\n",
      "[1150/1400] ok=1150 fail=0\n",
      "[1155/1400] ok=1155 fail=0\n",
      "[1160/1400] ok=1160 fail=0\n",
      "[1165/1400] ok=1165 fail=0\n",
      "[1170/1400] ok=1170 fail=0\n",
      "[1175/1400] ok=1175 fail=0\n",
      "[1180/1400] ok=1180 fail=0\n",
      "[1185/1400] ok=1185 fail=0\n",
      "[1190/1400] ok=1190 fail=0\n",
      "[1195/1400] ok=1195 fail=0\n",
      "[1200/1400] ok=1200 fail=0\n",
      "[1205/1400] ok=1205 fail=0\n",
      "[1210/1400] ok=1210 fail=0\n",
      "[1215/1400] ok=1215 fail=0\n",
      "[1220/1400] ok=1220 fail=0\n",
      "[1225/1400] ok=1225 fail=0\n",
      "[1230/1400] ok=1230 fail=0\n",
      "[1235/1400] ok=1235 fail=0\n",
      "[1240/1400] ok=1240 fail=0\n",
      "[1245/1400] ok=1245 fail=0\n",
      "[1250/1400] ok=1250 fail=0\n",
      "[1255/1400] ok=1255 fail=0\n",
      "[1260/1400] ok=1260 fail=0\n",
      "[1265/1400] ok=1265 fail=0\n",
      "[1270/1400] ok=1270 fail=0\n",
      "[1275/1400] ok=1275 fail=0\n",
      "[1280/1400] ok=1280 fail=0\n",
      "[1285/1400] ok=1285 fail=0\n",
      "[1290/1400] ok=1290 fail=0\n",
      "[1295/1400] ok=1295 fail=0\n",
      "[1300/1400] ok=1300 fail=0\n",
      "[1305/1400] ok=1305 fail=0\n",
      "[1310/1400] ok=1310 fail=0\n",
      "[1315/1400] ok=1315 fail=0\n",
      "[1320/1400] ok=1320 fail=0\n",
      "[1325/1400] ok=1325 fail=0\n",
      "[1330/1400] ok=1330 fail=0\n",
      "[1335/1400] ok=1335 fail=0\n",
      "[1340/1400] ok=1340 fail=0\n",
      "[1345/1400] ok=1345 fail=0\n",
      "[1350/1400] ok=1350 fail=0\n",
      "[1355/1400] ok=1355 fail=0\n",
      "[1360/1400] ok=1360 fail=0\n",
      "[1365/1400] ok=1365 fail=0\n",
      "[1370/1400] ok=1370 fail=0\n",
      "[1375/1400] ok=1375 fail=0\n",
      "[1380/1400] ok=1380 fail=0\n",
      "[1385/1400] ok=1385 fail=0\n",
      "[1390/1400] ok=1390 fail=0\n",
      "[1395/1400] ok=1395 fail=0\n",
      "[1400/1400] ok=1400 fail=0\n",
      "Wrote: /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/pipeline/nb_program_artifacts_type3/program_index.json\n",
      "Done in 1558.8s\n",
      "OK: 1400 FAILED: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
    "\n",
    "def _normalize_clone_types(clone_types: CloneTypes) -> List[str]:\n",
    "    if isinstance(clone_types, str):\n",
    "        return [clone_types]\n",
    "    return list(clone_types)\n",
    "\n",
    "def iter_program_sources(root: Path, clone_types: CloneTypes, limit_indices: Optional[int]) -> List[Path]:\n",
    "    idxs = list_indices(root)\n",
    "    if limit_indices is not None:\n",
    "        idxs = idxs[:limit_indices]\n",
    "\n",
    "    types = _normalize_clone_types(clone_types)\n",
    "\n",
    "    out: List[Path] = []\n",
    "    for idx in idxs:\n",
    "        a = anchor_path(root, idx)\n",
    "        if a.exists():\n",
    "            out.append(a)\n",
    "\n",
    "        for t in types:\n",
    "            for c in clone_paths(root, idx, t):\n",
    "                if c.exists():\n",
    "                    out.append(c)\n",
    "\n",
    "    # de-dupe\n",
    "    seen, uniq = set(), []\n",
    "    for p in out:\n",
    "        s = str(p.resolve())\n",
    "        if s not in seen:\n",
    "            seen.add(s)\n",
    "            uniq.append(p)\n",
    "    return uniq\n",
    "\n",
    "sources = iter_program_sources(DATASET_ROOT, CLONE_TYPES, LIMIT_INDICES)\n",
    "print(\"Programs to process:\", len(sources))\n",
    "\n",
    "program_index: Dict[str, Any] = {\"items\": {}, \"failures\": []}\n",
    "t0 = time.time()\n",
    "\n",
    "for i, src in enumerate(sources, 1):\n",
    "    try:\n",
    "        item = build_program_artifact(\n",
    "            src, OUT_DIR,\n",
    "            jdk_home=JDK_HOME,\n",
    "            vineflower_jar=VINEFLOWER_JAR,\n",
    "            model=model, tokenizer=tokenizer, device=DEVICE,\n",
    "            extra_javac=None,  # add flags later\n",
    "        )\n",
    "        program_index[\"items\"][str(Path(src).resolve())] = item\n",
    "    except Exception as e:\n",
    "        program_index[\"failures\"].append({\"source_path\": str(src), \"error\": str(e)})\n",
    "    if i % 5 == 0:\n",
    "        print(f\"[{i}/{len(sources)}] ok={len(program_index['items'])} fail={len(program_index['failures'])}\")\n",
    "\n",
    "program_index_path = OUT_DIR / \"program_index.json\"\n",
    "program_index_path.write_text(json.dumps(program_index, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote:\", program_index_path)\n",
    "print(\"Done in %.1fs\" % (time.time()-t0))\n",
    "print(\"OK:\", len(program_index[\"items\"]), \"FAILED:\", len(program_index[\"failures\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f04627",
   "metadata": {},
   "source": [
    "## 8) Program store loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9371169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program: /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/01/main.java\n",
      "num_methods: 6\n",
      "x shape: torch.Size([1, 768]) edge_index: torch.Size([2, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_program_methods(program_index: Dict[str,Any], program_src: str) -> Optional[List[Dict[str,Any]]]:\n",
    "    item = program_index[\"items\"].get(str(Path(program_src).resolve()))\n",
    "    if not item:\n",
    "        return None\n",
    "    shard = Path(item[\"embed_shard\"])\n",
    "    if not shard.exists():\n",
    "        return None\n",
    "    return torch.load(shard, map_location=\"cpu\")\n",
    "\n",
    "some_key = next(iter(program_index[\"items\"].keys()))\n",
    "methods = load_program_methods(program_index, some_key)\n",
    "print(\"Example program:\", some_key)\n",
    "print(\"num_methods:\", len(methods))\n",
    "print(\"x shape:\", methods[0][\"x\"].shape, \"edge_index:\", methods[0][\"edge_index\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19a8d5",
   "metadata": {},
   "source": [
    "## 9) Simple GNN encoder + program pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb6c87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scatter_mean(src: torch.Tensor, index: torch.Tensor, dim_size: int) -> torch.Tensor:\n",
    "    H = src.shape[-1]\n",
    "    out = torch.zeros((dim_size, H), device=src.device, dtype=src.dtype)\n",
    "    cnt = torch.zeros((dim_size, 1), device=src.device, dtype=src.dtype)\n",
    "    out.index_add_(0, index, src)\n",
    "    cnt.index_add_(0, index, torch.ones((index.shape[0],1), device=src.device, dtype=src.dtype))\n",
    "    return out / cnt.clamp(min=1.0)\n",
    "\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_dim: int = 768, hidden: int = 256, layers: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(in_dim, hidden)\n",
    "        self.layers = nn.ModuleList([nn.Linear(hidden, hidden) for _ in range(layers)])\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        h = F.relu(self.in_proj(x))\n",
    "        for lin in self.layers:\n",
    "            if edge_index.numel() == 0 or h.shape[0] == 0:\n",
    "                h = F.relu(lin(h))\n",
    "                continue\n",
    "            src, dst = edge_index[0], edge_index[1]\n",
    "            agg = scatter_mean(h[src], dst, dim_size=h.shape[0])\n",
    "            h = h + agg\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            h = F.relu(lin(h))\n",
    "        return h\n",
    "\n",
    "def encode_method(gnn: SimpleGNN, rec: Dict[str,Any], device: str) -> torch.Tensor:\n",
    "    x = rec[\"x\"].to(device)\n",
    "    ei = rec[\"edge_index\"].to(device)\n",
    "    if x.shape[0] == 0:\n",
    "        return torch.zeros((gnn.in_proj.out_features,), device=device)\n",
    "    h = gnn(x, ei)\n",
    "    return h.mean(dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_program_mean(gnn: SimpleGNN, method_recs: List[Dict[str,Any]], device: str) -> torch.Tensor:\n",
    "    if not method_recs:\n",
    "        return torch.zeros((gnn.in_proj.out_features,), device=device)\n",
    "    embs = [encode_method(gnn, rec, device) for rec in method_recs]\n",
    "    return torch.stack(embs, dim=0).mean(dim=0)\n",
    "\n",
    "class PairClassifier(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim * 2, dim)\n",
    "        self.fc2 = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, ha: torch.Tensor, hb: torch.Tensor) -> torch.Tensor:\n",
    "        feat = torch.cat([torch.abs(ha - hb), ha * hb], dim=-1)\n",
    "        z = F.relu(self.fc1(feat))\n",
    "        return self.fc2(z).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87f2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception:\n",
    "    plt = None\n",
    "\n",
    "\n",
    "def _auc_roc_from_scores(y_true: List[int], y_score: List[float]) -> Tuple[float, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Pure-python ROC AUC (no sklearn dependency).\n",
    "    Returns: (auc, fpr_list, tpr_list)\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_score) and len(y_true) > 0\n",
    "\n",
    "    # Count positives/negatives\n",
    "    p = sum(1 for y in y_true if y == 1)\n",
    "    n = len(y_true) - p\n",
    "    if p == 0 or n == 0:\n",
    "        return float(\"nan\"), [], []\n",
    "\n",
    "    # Sort by score descending\n",
    "    pairs = sorted(zip(y_score, y_true), key=lambda t: t[0], reverse=True)\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tpr = [0.0]\n",
    "    fpr = [0.0]\n",
    "\n",
    "    # Walk thresholds (at each unique score)\n",
    "    last_score = None\n",
    "    for score, y in pairs:\n",
    "        if last_score is None:\n",
    "            last_score = score\n",
    "\n",
    "        # If score changed, record point before stepping into next threshold\n",
    "        if score != last_score:\n",
    "            tpr.append(tp / p)\n",
    "            fpr.append(fp / n)\n",
    "            last_score = score\n",
    "\n",
    "        if y == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    # Final point\n",
    "    tpr.append(tp / p)\n",
    "    fpr.append(fp / n)\n",
    "\n",
    "    # Trapezoidal area\n",
    "    auc = 0.0\n",
    "    for i in range(1, len(fpr)):\n",
    "        auc += (fpr[i] - fpr[i - 1]) * (tpr[i] + tpr[i - 1]) / 2.0\n",
    "\n",
    "    return float(auc), fpr, tpr\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_stream_auc(\n",
    "    gnn,\n",
    "    clf,\n",
    "    program_index: Dict[str, Any],\n",
    "    pair_it: Iterator[Tuple[str, str, int]],\n",
    "    device: str,\n",
    "    num_pairs: int,\n",
    "    *,\n",
    "    plot: bool = False,\n",
    "    title: str = \"ROC (val)\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluates `num_pairs` from pair_it.\n",
    "    Returns val_loss, val_acc, auc, and bookkeeping counts.\n",
    "\n",
    "    Uses y_score = sigmoid(logit) for ROC AUC.\n",
    "    \"\"\"\n",
    "    gnn.eval()\n",
    "    clf.eval()\n",
    "\n",
    "    losses: List[torch.Tensor] = []\n",
    "    y_true: List[int] = []\n",
    "    y_score: List[float] = []\n",
    "\n",
    "    used = 0\n",
    "    pos = 0\n",
    "    correct = 0\n",
    "\n",
    "    for _ in range(num_pairs):\n",
    "        a, b, y = next(pair_it)\n",
    "\n",
    "        ma = load_program_methods(program_index, a)\n",
    "        mb = load_program_methods(program_index, b)\n",
    "        if ma is None or mb is None:\n",
    "            continue\n",
    "\n",
    "        ha = encode_program_mean(gnn, ma, device)\n",
    "        hb = encode_program_mean(gnn, mb, device)\n",
    "\n",
    "        logit = clf(ha, hb).view(())  # scalar\n",
    "        target = torch.tensor(float(y), device=device)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(logit, target)\n",
    "        prob = torch.sigmoid(logit).item()\n",
    "\n",
    "        pred = 1 if prob >= 0.5 else 0\n",
    "        correct += int(pred == int(y))\n",
    "\n",
    "        used += 1\n",
    "        pos += int(y == 1)\n",
    "        losses.append(loss.detach().cpu())\n",
    "        y_true.append(int(y))\n",
    "        y_score.append(float(prob))\n",
    "\n",
    "    if used == 0:\n",
    "        return {\n",
    "            \"val_loss\": None,\n",
    "            \"val_acc\": None,\n",
    "            \"auc\": None,\n",
    "            \"used\": 0,\n",
    "            \"pos\": 0,\n",
    "            \"neg\": 0,\n",
    "        }\n",
    "\n",
    "    val_loss = float(torch.stack(losses).mean())\n",
    "    val_acc = correct / used\n",
    "    auc, fpr, tpr = _auc_roc_from_scores(y_true, y_score)\n",
    "\n",
    "    if plot and plt is not None and fpr and tpr and not math.isnan(auc):\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"{title} | AUC={auc:.3f} | used={used} pos={pos} neg={used-pos}\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"auc\": auc if not math.isnan(auc) else None,\n",
    "        \"used\": used,\n",
    "        \"pos\": pos,\n",
    "        \"neg\": used - pos,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8f92e",
   "metadata": {},
   "source": [
    "## 10) Training loop (program-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c5ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainCfg:\n",
    "    steps: int = 2000\n",
    "    batch_pairs: int = 32\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    log_every: int = 50\n",
    "    eval_every: int = 200\n",
    "    val_pairs: int = 200\n",
    "    pos_ratio: float = 0.5\n",
    "    hidden: int = 256\n",
    "    layers: int = 3\n",
    "    dropout: float = 0.1\n",
    "    val_ratio: float = 0.2\n",
    "\n",
    "cfg = TrainCfg()\n",
    "\n",
    "def batch_k(it: Iterator[Tuple[str,str,int]], k: int) -> List[Tuple[str,str,int]]:\n",
    "    return [next(it) for _ in range(k)]\n",
    "\n",
    "def eval_stream(gnn, clf, program_index, pair_it, device: str, num_pairs: int) -> Dict[str, Any]:\n",
    "    gnn.eval(); clf.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    used = 0\n",
    "    pos = 0\n",
    "    for _ in range(num_pairs):\n",
    "        a,b,y = next(pair_it)\n",
    "        ma = load_program_methods(program_index, a)\n",
    "        mb = load_program_methods(program_index, b)\n",
    "        if ma is None or mb is None:\n",
    "            continue\n",
    "        ha = encode_program_mean(gnn, ma, device)\n",
    "        hb = encode_program_mean(gnn, mb, device)\n",
    "        logit = clf(ha, hb)\n",
    "        target = torch.tensor(float(y), device=device)\n",
    "        loss = F.binary_cross_entropy_with_logits(logit, target)\n",
    "        prob = torch.sigmoid(logit).item()\n",
    "        pred = 1 if prob >= 0.5 else 0\n",
    "        correct += int(pred == int(y))\n",
    "        pos += int(y == 1)\n",
    "        used += 1\n",
    "        losses.append(loss.detach().cpu())\n",
    "    if used == 0:\n",
    "        return {\"val_loss\": None, \"val_acc\": None, \"used\": 0, \"pos\": 0, \"neg\": 0}\n",
    "    return {\n",
    "        \"val_loss\": float(torch.stack(losses).mean()),\n",
    "        \"val_acc\": correct / used,\n",
    "        \"used\": used,\n",
    "        \"pos\": pos,\n",
    "        \"neg\": used - pos,\n",
    "    }\n",
    "\n",
    "def train_program_model(program_index: Dict[str,Any],\n",
    "                        train_it: Iterator[Tuple[str,str,int]],\n",
    "                        val_it: Iterator[Tuple[str,str,int]],\n",
    "                        device: str,\n",
    "                        cfg: TrainCfg):\n",
    "    torch.manual_seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    gnn = SimpleGNN(in_dim=768, hidden=cfg.hidden, layers=cfg.layers, dropout=cfg.dropout).to(device)\n",
    "    clf = PairClassifier(dim=cfg.hidden).to(device)\n",
    "    opt = torch.optim.AdamW(list(gnn.parameters()) + list(clf.parameters()), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    hist = []\n",
    "    ema = None\n",
    "    gnn.train(); clf.train()\n",
    "\n",
    "    for step in range(1, cfg.steps + 1):\n",
    "        batch = batch_k(train_it, cfg.batch_pairs)\n",
    "        logits, targets = [], []\n",
    "        for a,b,y in batch:\n",
    "            ma = load_program_methods(program_index, a)\n",
    "            mb = load_program_methods(program_index, b)\n",
    "            if ma is None or mb is None:\n",
    "                continue\n",
    "            ha = encode_program_mean(gnn, ma, device)\n",
    "            hb = encode_program_mean(gnn, mb, device)\n",
    "            logits.append(clf(ha, hb))\n",
    "            targets.append(torch.tensor(float(y), device=device))\n",
    "        if not logits:\n",
    "            continue\n",
    "\n",
    "        logits_t = torch.stack(logits)\n",
    "        targets_t = torch.stack(targets)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits_t, targets_t)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        l = float(loss.detach().cpu())\n",
    "        ema = l if ema is None else 0.98 * ema + 0.02 * l\n",
    "\n",
    "        if step % cfg.log_every == 0:\n",
    "            print(f\"step={step} loss={l:.4f} ema={ema:.4f}\")\n",
    "\n",
    "        if step % cfg.eval_every == 0:\n",
    "            metrics = eval_stream_auc(\n",
    "                gnn, clf, program_index, val_it,\n",
    "                device=device,\n",
    "                num_pairs=cfg.val_pairs,\n",
    "                plot=False,               # set True to draw ROC every eval\n",
    "                title=f\"ROC at step {step}\",\n",
    "            )\n",
    "            print(\n",
    "                f\"[VAL] used={metrics['used']} pos={metrics['pos']} neg={metrics['neg']} \"\n",
    "                f\"val_loss={metrics['val_loss']} val_acc={metrics['val_acc']} auc={metrics['auc']}\"\n",
    "            )\n",
    "            hist.append({\"step\": step, \"train_loss\": l, \"ema\": ema, **metrics})\n",
    "            gnn.train(); clf.train()\n",
    "\n",
    "    return gnn, clf, hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87268dd0",
   "metadata": {},
   "source": [
    "## 11) Train/val split + training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cda4e041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_idxs: 160 val_idxs: 40\n",
      "step=50 loss=0.6362 ema=0.6790\n",
      "step=100 loss=0.5884 ema=0.6434\n",
      "step=150 loss=0.6212 ema=0.6225\n",
      "step=200 loss=0.6184 ema=0.6001\n",
      "[VAL] used=200 pos=114 neg=86 val_loss=0.5397405028343201 val_acc=0.74 auc=0.8311913504691963\n",
      "step=250 loss=0.4973 ema=0.5875\n",
      "step=300 loss=0.4767 ema=0.5633\n",
      "step=350 loss=0.5244 ema=0.5580\n",
      "step=400 loss=0.6479 ema=0.5423\n",
      "[VAL] used=200 pos=120 neg=80 val_loss=0.507855236530304 val_acc=0.7 auc=0.8275000000000002\n",
      "step=450 loss=0.4540 ema=0.5333\n",
      "step=500 loss=0.4804 ema=0.5015\n",
      "step=550 loss=0.3640 ema=0.4874\n",
      "step=600 loss=0.4264 ema=0.4728\n",
      "[VAL] used=200 pos=102 neg=98 val_loss=0.5174974203109741 val_acc=0.75 auc=0.9031612645058024\n",
      "step=650 loss=0.4787 ema=0.4908\n",
      "step=700 loss=0.3972 ema=0.4896\n",
      "step=750 loss=0.3666 ema=0.4837\n",
      "step=800 loss=0.5729 ema=0.4696\n",
      "[VAL] used=200 pos=95 neg=105 val_loss=0.6347355246543884 val_acc=0.695 auc=0.9048621553884708\n",
      "step=850 loss=0.6371 ema=0.4546\n",
      "step=900 loss=0.4989 ema=0.4563\n",
      "step=950 loss=0.4295 ema=0.4642\n",
      "step=1000 loss=0.4095 ema=0.4629\n",
      "[VAL] used=200 pos=113 neg=87 val_loss=0.3877049386501312 val_acc=0.845 auc=0.9390702878649175\n",
      "step=1050 loss=0.4802 ema=0.4580\n",
      "step=1100 loss=0.3154 ema=0.4478\n",
      "step=1150 loss=0.6204 ema=0.4587\n",
      "step=1200 loss=0.3547 ema=0.4496\n",
      "[VAL] used=200 pos=100 neg=100 val_loss=0.46878477931022644 val_acc=0.79 auc=0.9075000000000004\n",
      "step=1250 loss=0.4748 ema=0.4449\n",
      "step=1300 loss=0.3237 ema=0.4430\n",
      "step=1350 loss=0.4098 ema=0.4241\n",
      "step=1400 loss=0.4777 ema=0.4233\n",
      "[VAL] used=200 pos=102 neg=98 val_loss=0.5045282244682312 val_acc=0.79 auc=0.9203681472589036\n",
      "step=1450 loss=0.5556 ema=0.4306\n",
      "step=1500 loss=0.5726 ema=0.4370\n",
      "step=1550 loss=0.3918 ema=0.4386\n",
      "step=1600 loss=0.3292 ema=0.4337\n",
      "[VAL] used=200 pos=103 neg=97 val_loss=0.4648929536342621 val_acc=0.795 auc=0.9381443298969071\n",
      "step=1650 loss=0.3716 ema=0.4339\n",
      "step=1700 loss=0.4514 ema=0.4288\n",
      "step=1750 loss=0.4032 ema=0.4365\n",
      "step=1800 loss=0.3797 ema=0.4532\n",
      "[VAL] used=200 pos=100 neg=100 val_loss=0.39057961106300354 val_acc=0.855 auc=0.9493999999999998\n",
      "step=1850 loss=0.5993 ema=0.4310\n",
      "step=1900 loss=0.4309 ema=0.4228\n",
      "step=1950 loss=0.6188 ema=0.4345\n",
      "step=2000 loss=0.3765 ema=0.4243\n",
      "[VAL] used=200 pos=97 neg=103 val_loss=0.5680042505264282 val_acc=0.78 auc=0.9172255029526578\n"
     ]
    }
   ],
   "source": [
    "def extract_idx_from_synthetic_path(p: str) -> Optional[str]:\n",
    "    parts = Path(p).parts\n",
    "    for i, seg in enumerate(parts):\n",
    "        if seg == \"base\" or seg.startswith(\"type-\"):\n",
    "            if i + 1 < len(parts):\n",
    "                return parts[i + 1]\n",
    "    return None\n",
    "\n",
    "def split_indices(idxs: List[str], val_ratio: float, seed: int) -> Tuple[Set[str], Set[str]]:\n",
    "    rng = random.Random(seed)\n",
    "    idxs = idxs[:]\n",
    "    rng.shuffle(idxs)\n",
    "    n_val = max(1, int(round(len(idxs) * val_ratio))) if idxs else 0\n",
    "    val = set(idxs[:n_val])\n",
    "    train = set(idxs[n_val:])\n",
    "    return train, val\n",
    "\n",
    "def filter_pairs_by_anchor_index(pair_it, allowed: Set[str]) -> Iterator[Tuple[str, str, int]]:\n",
    "    while True:\n",
    "        a, b, y = next(pair_it)\n",
    "        ia = extract_idx_from_synthetic_path(a)\n",
    "        if ia is None:\n",
    "            continue\n",
    "        if ia in allowed:\n",
    "            yield (a, b, y)\n",
    "\n",
    "def positive_pairs_multi(\n",
    "    *,\n",
    "    root: Path,\n",
    "    clone_types: List[str],\n",
    "    seed: int = 0,\n",
    "    limit_indices: Optional[int] = None,\n",
    "    infinite: bool = True,\n",
    ") -> Iterator[Tuple[str, str, int]]:\n",
    "    rng = random.Random(seed)\n",
    "    idxs = list_indices(root)\n",
    "    if limit_indices is not None:\n",
    "        idxs = idxs[:limit_indices]\n",
    "\n",
    "    per: List[Tuple[str, Path, List[Path]]] = []\n",
    "    for idx in idxs:\n",
    "        a = anchor_path(root, idx)\n",
    "        if not a.exists():\n",
    "            continue\n",
    "\n",
    "        cs: List[Path] = []\n",
    "        for ct in clone_types:\n",
    "            cs.extend(clone_paths(root, idx, ct))\n",
    "\n",
    "        cs = [p for p in cs if p.exists()]\n",
    "        if cs:\n",
    "            per.append((idx, a, cs))\n",
    "\n",
    "    if not per:\n",
    "        raise RuntimeError(\"No positive pairs found. Check base/ and clone type folders.\")\n",
    "\n",
    "    while True:\n",
    "        _, a, cs = rng.choice(per)\n",
    "        b = rng.choice(cs)\n",
    "        yield (str(a), str(b), 1)\n",
    "        if not infinite:\n",
    "            return\n",
    "\n",
    "def negative_pairs_multi(\n",
    "    *,\n",
    "    root: Path,\n",
    "    clone_types: List[str],\n",
    "    seed: int = 0,\n",
    "    limit_indices: Optional[int] = None,\n",
    "    infinite: bool = True,\n",
    ") -> Iterator[Tuple[str, str, int]]:\n",
    "    rng = random.Random(seed + 12345)\n",
    "    idxs = list_indices(root)\n",
    "    if limit_indices is not None:\n",
    "        idxs = idxs[:limit_indices]\n",
    "\n",
    "    anchors = [(idx, anchor_path(root, idx)) for idx in idxs]\n",
    "    anchors = [(idx, p) for idx, p in anchors if p.exists()]\n",
    "\n",
    "    clone_pool: List[Tuple[str, Path]] = []\n",
    "    for idx in idxs:\n",
    "        for ct in clone_types:\n",
    "            for p in clone_paths(root, idx, ct):\n",
    "                if p.exists():\n",
    "                    clone_pool.append((idx, p))\n",
    "\n",
    "    if not anchors or not clone_pool:\n",
    "        raise RuntimeError(\"No negative pools found. Check clone folders.\")\n",
    "\n",
    "    while True:\n",
    "        idx_a, a = rng.choice(anchors)\n",
    "        while True:\n",
    "            idx_b, b = rng.choice(clone_pool)\n",
    "            if idx_b != idx_a:\n",
    "                break\n",
    "        yield (str(a), str(b), 0)\n",
    "        if not infinite:\n",
    "            return\n",
    "\n",
    "all_idxs = list_indices(DATASET_ROOT)\n",
    "if LIMIT_INDICES is not None:\n",
    "    all_idxs = all_idxs[:LIMIT_INDICES]\n",
    "\n",
    "train_idxs, val_idxs = split_indices(all_idxs, val_ratio=cfg.val_ratio, seed=SEED)\n",
    "print(\"train_idxs:\", len(train_idxs), \"val_idxs:\", len(val_idxs))\n",
    "\n",
    "pos = positive_pairs_multi(root=DATASET_ROOT, clone_types=CLONE_TYPES, seed=SEED, limit_indices=LIMIT_INDICES)\n",
    "neg = negative_pairs_multi(root=DATASET_ROOT, clone_types=CLONE_TYPES, seed=SEED, limit_indices=LIMIT_INDICES)\n",
    "mix = interleave(pos, neg, pos_ratio=cfg.pos_ratio, seed=SEED)\n",
    "\n",
    "pos_v = positive_pairs_multi(root=DATASET_ROOT, clone_types=CLONE_TYPES, seed=SEED+999, limit_indices=LIMIT_INDICES)\n",
    "neg_v = negative_pairs_multi(root=DATASET_ROOT, clone_types=CLONE_TYPES, seed=SEED+999, limit_indices=LIMIT_INDICES)\n",
    "mix_v = interleave(pos_v, neg_v, pos_ratio=cfg.pos_ratio, seed=SEED+999)\n",
    "\n",
    "train_it = filter_pairs_by_anchor_index(mix, train_idxs)\n",
    "val_it = filter_pairs_by_anchor_index(mix_v, val_idxs)\n",
    "\n",
    "gnn, clf, hist = train_program_model(program_index, train_it, val_it, device=DEVICE, cfg=cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f01c4",
   "metadata": {},
   "source": [
    "## 12) Inspect last validation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7b1b046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step': 1200, 'train_loss': 0.35469865798950195, 'ema': 0.449569697398415, 'val_loss': 0.46878477931022644, 'val_acc': 0.79, 'auc': 0.9075000000000004, 'used': 200, 'pos': 100, 'neg': 100}\n",
      "{'step': 1400, 'train_loss': 0.4776732325553894, 'ema': 0.42333324633304215, 'val_loss': 0.5045282244682312, 'val_acc': 0.79, 'auc': 0.9203681472589036, 'used': 200, 'pos': 102, 'neg': 98}\n",
      "{'step': 1600, 'train_loss': 0.3291569948196411, 'ema': 0.43372565692224757, 'val_loss': 0.4648929536342621, 'val_acc': 0.795, 'auc': 0.9381443298969071, 'used': 200, 'pos': 103, 'neg': 97}\n",
      "{'step': 1800, 'train_loss': 0.3797124922275543, 'ema': 0.45317501048200143, 'val_loss': 0.39057961106300354, 'val_acc': 0.855, 'auc': 0.9493999999999998, 'used': 200, 'pos': 100, 'neg': 100}\n",
      "{'step': 2000, 'train_loss': 0.37648457288742065, 'ema': 0.4243469587609075, 'val_loss': 0.5680042505264282, 'val_acc': 0.78, 'auc': 0.9172255029526578, 'used': 200, 'pos': 97, 'neg': 103}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if hist:\n",
    "    for r in hist[-5:]:\n",
    "        print(r)\n",
    "else:\n",
    "    print(\"No eval points recorded yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f46b678c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.4665950536727905,\n",
       " 'val_acc': 0.804,\n",
       " 'auc': 0.9364971279226887,\n",
       " 'used': 2000,\n",
       " 'pos': 1046,\n",
       " 'neg': 954}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training: fresh validation stream for final ROC/AUC\n",
    "pos_v2 = positive_pairs(root=DATASET_ROOT, clone_types=CLONE_TYPES, seed=SEED+999, limit_indices=LIMIT_INDICES)\n",
    "neg_v2 = negative_pairs(root=DATASET_ROOT, clone_types=CLONE_TYPES, seed=SEED+999, limit_indices=LIMIT_INDICES)\n",
    "mix_v2 = interleave(pos_v2, neg_v2, pos_ratio=cfg.pos_ratio, seed=SEED+999)\n",
    "val_it2 = filter_pairs_by_anchor_index(mix_v2, val_idxs)\n",
    "\n",
    "final = eval_stream_auc(gnn, clf, program_index, val_it2, device=DEVICE, num_pairs=2000, plot=True, title=\"Final ROC (val)\")\n",
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
