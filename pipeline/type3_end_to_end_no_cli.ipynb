{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553e98cc",
   "metadata": {},
   "source": [
    "# End-to-end (pure Python) pipeline: Type-3 clone dataset → compile/decompile → AST graphs → embeddings → GNN training\n",
    "\n",
    "This notebook runs the full pipeline **without using any `python -m ...cli` commands**. It builds program artifacts from the **synthetic Code-Clone dataset** (`base/`, `type-1/2/3/`), generates method graphs from **decompiled** code, embeds nodes with **GraphCodeBERT**, and trains a small **graph encoder + program pooling + pair classifier**.\n",
    "\n",
    "Notes:\n",
    "- Uses Tree-sitter Java for parsing and a lightweight graph schema: **SEQ**, **AST**, **IF_THEN**, **IF_ELSE**.\n",
    "- Uses a small message-passing GNN implemented directly in PyTorch (no PyG dependency).\n",
    "- You can switch to training on original source instead of decompiled by toggling `USE_DECOMPILED`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e946303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_java\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d686e",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc52673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_ROOT: /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset\n",
      "OUT_DIR     : /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/pipeline/nb_program_artifacts_type3\n",
      "DEVICE      : mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Dataset ----\n",
    "DATASET_ROOT = Path(\"../data/code-clone-dataset/dataset\").expanduser().resolve()\n",
    "CLONE_TYPE = \"type-3\"       # \"type-1\" | \"type-2\" | \"type-3\"\n",
    "LIMIT_INDICES: Optional[int] = 10   # set None for all\n",
    "SEED = 0\n",
    "\n",
    "# ---- Tools ----\n",
    "JDK_HOME = Path(os.environ.get(\"JAVA_HOME\", \"\")).expanduser()\n",
    "VINEFLOWER_JAR = Path(\"../chatgpt/vineflower-1.11.2.jar\").expanduser().resolve()\n",
    "\n",
    "# ---- Output ----\n",
    "OUT_DIR = Path(\"./nb_program_artifacts_type3\").resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Embedding model ----\n",
    "MODEL_NAME = \"microsoft/graphcodebert-base\"\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ---- Pipeline toggles ----\n",
    "USE_DECOMPILED = True      # If True: compile->jar->decompile and graph decompiled java\n",
    "FORCE_REBUILD = False      # If True: delete and rebuild existing program artifacts\n",
    "\n",
    "print(\"DATASET_ROOT:\", DATASET_ROOT)\n",
    "print(\"OUT_DIR     :\", OUT_DIR)\n",
    "print(\"DEVICE      :\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a941b",
   "metadata": {},
   "source": [
    "## 2) Dataset enumeration and pair generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f50eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/07/main.java -> /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/type-3/08/3.java\n",
      "1 0 /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/01/main.java -> /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/type-3/09/3.java\n",
      "2 1 /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/07/main.java -> /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/type-3/07/2.java\n",
      "3 1 /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/01/main.java -> /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/type-3/01/2.java\n",
      "4 0 /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/05/main.java -> /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/type-3/10/1.java\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def list_indices(root: Path) -> List[str]:\n",
    "    base = root / \"base\"\n",
    "    idxs = []\n",
    "    for p in sorted(base.iterdir()):\n",
    "        if p.is_dir():\n",
    "            idxs.append(p.name)\n",
    "    return idxs\n",
    "\n",
    "def anchor_path(root: Path, idx: str) -> Path:\n",
    "    return root / \"base\" / idx / \"main.java\"\n",
    "\n",
    "def clone_paths(root: Path, idx: str, clone_type: str) -> List[Path]:\n",
    "    d = root / clone_type / idx\n",
    "    if not d.exists():\n",
    "        return []\n",
    "    return sorted(d.glob(\"*.java\"))\n",
    "\n",
    "def positive_pairs(*, root: Path, clone_type: str, seed: int = 0,\n",
    "                   limit_indices: Optional[int] = None, infinite: bool = True) -> Iterator[Tuple[str,str,int]]:\n",
    "    rng = random.Random(seed)\n",
    "    idxs = list_indices(root)\n",
    "    if limit_indices is not None:\n",
    "        idxs = idxs[:limit_indices]\n",
    "    per = []\n",
    "    for idx in idxs:\n",
    "        a = anchor_path(root, idx)\n",
    "        cs = clone_paths(root, idx, clone_type)\n",
    "        if a.exists() and cs:\n",
    "            per.append((idx, a, cs))\n",
    "    if not per:\n",
    "        raise RuntimeError(\"No positive pairs found. Check DATASET_ROOT/CLONE_TYPE.\")\n",
    "    while True:\n",
    "        idx, a, cs = rng.choice(per)\n",
    "        b = rng.choice(cs)\n",
    "        yield (str(a), str(b), 1)\n",
    "        if not infinite:\n",
    "            return\n",
    "\n",
    "def negative_pairs(*, root: Path, clone_type: str, seed: int = 0,\n",
    "                   limit_indices: Optional[int] = None, infinite: bool = True) -> Iterator[Tuple[str,str,int]]:\n",
    "    rng = random.Random(seed + 12345)\n",
    "    idxs = list_indices(root)\n",
    "    if limit_indices is not None:\n",
    "        idxs = idxs[:limit_indices]\n",
    "    anchors = [(idx, anchor_path(root, idx)) for idx in idxs]\n",
    "    anchors = [(idx, p) for idx,p in anchors if p.exists()]\n",
    "    clone_pool = []\n",
    "    for idx in idxs:\n",
    "        for p in clone_paths(root, idx, clone_type):\n",
    "            if p.exists():\n",
    "                clone_pool.append((idx, p))\n",
    "    if not anchors or not clone_pool:\n",
    "        raise RuntimeError(\"No negative pools found.\")\n",
    "    while True:\n",
    "        idx_a, a = rng.choice(anchors)\n",
    "        while True:\n",
    "            idx_b, b = rng.choice(clone_pool)\n",
    "            if idx_b != idx_a:\n",
    "                break\n",
    "        yield (str(a), str(b), 0)\n",
    "        if not infinite:\n",
    "            return\n",
    "\n",
    "def interleave(pos_it, neg_it, pos_ratio: float = 0.5, seed: int = 0) -> Iterator[Tuple[str,str,int]]:\n",
    "    rng = random.Random(seed)\n",
    "    while True:\n",
    "        if rng.random() < pos_ratio:\n",
    "            yield next(pos_it)\n",
    "        else:\n",
    "            yield next(neg_it)\n",
    "\n",
    "# Preview\n",
    "pos = positive_pairs(root=DATASET_ROOT, clone_type=CLONE_TYPE, seed=SEED, limit_indices=LIMIT_INDICES)\n",
    "neg = negative_pairs(root=DATASET_ROOT, clone_type=CLONE_TYPE, seed=SEED, limit_indices=LIMIT_INDICES)\n",
    "mix = interleave(pos, neg, pos_ratio=0.5, seed=SEED)\n",
    "\n",
    "for i in range(5):\n",
    "    a,b,y = next(mix)\n",
    "    print(i, y, a, \"->\", b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc71b8",
   "metadata": {},
   "source": [
    "## 3) Java compile + jar + decompile helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b82aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLASS_RE = re.compile(r\"public\\s+class\\s+([A-Za-z_]\\w*)\")\n",
    "\n",
    "def detect_public_class_name(java_text: str) -> Optional[str]:\n",
    "    m = CLASS_RE.search(java_text)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def run(cmd: List[str], cwd: Optional[Path] = None, timeout: int = 300) -> Tuple[int,str,str]:\n",
    "    p = subprocess.run(\n",
    "        cmd,\n",
    "        cwd=str(cwd) if cwd else None,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    return p.returncode, p.stdout, p.stderr\n",
    "\n",
    "def compile_single_java(java_path: Path, work_dir: Path, jdk_home: Path, extra_javac: Optional[List[str]] = None) -> Path:\n",
    "    work_dir.mkdir(parents=True, exist_ok=True)\n",
    "    src_dir = work_dir / \"src\"\n",
    "    cls_dir = work_dir / \"classes\"\n",
    "    src_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cls_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    text = java_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    cls = detect_public_class_name(text) or java_path.stem\n",
    "    target_java = src_dir / f\"{cls}.java\"\n",
    "    target_java.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "    javac = (jdk_home / \"bin\" / \"javac\") if jdk_home and (jdk_home / \"bin\" / \"javac\").exists() else Path(\"javac\")\n",
    "    flags = [\"-g\", \"-encoding\", \"UTF-8\", \"-d\", str(cls_dir)]\n",
    "    if extra_javac:\n",
    "        flags.extend(extra_javac)\n",
    "\n",
    "    rc, out, err = run([str(javac), *flags, str(target_java)], cwd=work_dir, timeout=300)\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"javac failed\\n{err}\\n\")\n",
    "\n",
    "    jar = (jdk_home / \"bin\" / \"jar\") if jdk_home and (jdk_home / \"bin\" / \"jar\").exists() else Path(\"jar\")\n",
    "    jar_path = work_dir / \"app.jar\"\n",
    "    rc, out, err = run([str(jar), \"--create\", \"--file\", str(jar_path), \"-C\", str(cls_dir), \".\"], cwd=work_dir, timeout=300)\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"jar failed\\n{err}\\n\")\n",
    "    return jar_path\n",
    "\n",
    "def decompile_jar(jar_path: Path, out_dir: Path, vineflower_jar: Path) -> Path:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    rc, out, err = run([\"java\", \"-jar\", str(vineflower_jar), str(jar_path), str(out_dir)], cwd=out_dir, timeout=300)\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"vineflower failed\\n{err}\\n\")\n",
    "    return out_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c025a5",
   "metadata": {},
   "source": [
    "## 4) Tree-sitter Java parser + lightweight AST graph builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f34ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes: 5 edges: 12 edge_types: ['AST', 'IF_ELSE', 'IF_THEN', 'SEQ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "JAVA_LANGUAGE = Language(tree_sitter_java.language())\n",
    "parser = Parser(JAVA_LANGUAGE)\n",
    "\n",
    "CONTROL_NODES = {\n",
    "    \"if_statement\",\"for_statement\",\"while_statement\",\"do_statement\",\"switch_statement\",\n",
    "    \"try_statement\",\"catch_clause\",\n",
    "}\n",
    "STRAIGHT_NODES = {\n",
    "    \"local_variable_declaration\",\"expression_statement\",\"return_statement\",\"throw_statement\",\n",
    "}\n",
    "\n",
    "EDGE_TYPE_TO_ID = {\"SEQ\":0, \"AST\":1, \"IF_THEN\":2, \"IF_ELSE\":3}\n",
    "\n",
    "def _safe_text(code_bytes: bytes, node) -> str:\n",
    "    return code_bytes[node.start_byte:node.end_byte].decode(\"utf-8\", errors=\"replace\").strip()\n",
    "\n",
    "def extract_control_header(code: bytes, node) -> str:\n",
    "    for child in node.children:\n",
    "        if child.type in (\"condition\", \"parenthesized_expression\"):\n",
    "            return code[child.start_byte:child.end_byte].decode(\"utf-8\", errors=\"replace\").strip()\n",
    "    text = code[node.start_byte:node.end_byte].decode(\"utf-8\", errors=\"replace\")\n",
    "    return text.split(\"{\")[0].strip()\n",
    "\n",
    "@dataclass\n",
    "class GraphNode:\n",
    "    id: int\n",
    "    kind: str               # \"control\" | \"straight\"\n",
    "    ast_type: str\n",
    "    code: str\n",
    "    start_byte: int\n",
    "    end_byte: int\n",
    "    depth: int\n",
    "\n",
    "@dataclass\n",
    "class GraphEdge:\n",
    "    src: int\n",
    "    dst: int\n",
    "    type: str               # SEQ | AST | IF_THEN | IF_ELSE\n",
    "\n",
    "def build_method_graph(java_source: str) -> Dict[str, Any]:\n",
    "    code_bytes = java_source.encode(\"utf-8\")\n",
    "    tree = parser.parse(code_bytes)\n",
    "\n",
    "    nodes: List[GraphNode] = []\n",
    "    edges: List[GraphEdge] = []\n",
    "    next_id = 0\n",
    "    span_to_gid: Dict[Tuple[int,int,str], int] = {}\n",
    "\n",
    "    def add_node(ts_node, depth: int) -> Optional[int]:\n",
    "        nonlocal next_id\n",
    "        t = ts_node.type\n",
    "        is_control = t in CONTROL_NODES\n",
    "        is_straight = t in STRAIGHT_NODES\n",
    "        if not (is_control or is_straight):\n",
    "            return None\n",
    "        if is_control:\n",
    "            code = extract_control_header(code_bytes, ts_node)\n",
    "            kind = \"control\"\n",
    "        else:\n",
    "            code = _safe_text(code_bytes, ts_node)\n",
    "            kind = \"straight\"\n",
    "        if not code:\n",
    "            return None\n",
    "        gid = next_id; next_id += 1\n",
    "        nodes.append(GraphNode(\n",
    "            id=gid, kind=kind, ast_type=t, code=code,\n",
    "            start_byte=ts_node.start_byte, end_byte=ts_node.end_byte, depth=depth\n",
    "        ))\n",
    "        span_to_gid[(ts_node.start_byte, ts_node.end_byte, ts_node.type)] = gid\n",
    "        return gid\n",
    "\n",
    "    stack: List[Tuple[Any,int,Optional[int]]] = [(tree.root_node, 0, None)]\n",
    "    while stack:\n",
    "        ts_node, depth, parent_sel = stack.pop()\n",
    "        cur = add_node(ts_node, depth)\n",
    "        next_parent = parent_sel\n",
    "        if cur is not None:\n",
    "            if parent_sel is not None:\n",
    "                edges.append(GraphEdge(parent_sel, cur, \"AST\"))\n",
    "                edges.append(GraphEdge(cur, parent_sel, \"AST\"))\n",
    "            next_parent = cur\n",
    "        for ch in reversed(ts_node.children):\n",
    "            if len(ch.children) == 0 and ch.type in (\";\", \"{\", \"}\", \"(\", \")\", \",\"):\n",
    "                continue\n",
    "            stack.append((ch, depth+1, next_parent))\n",
    "\n",
    "    if len(nodes) >= 2:\n",
    "        top_depth = min(n.depth for n in nodes)\n",
    "        top_nodes = [n for n in nodes if n.depth == top_depth]\n",
    "        top_nodes.sort(key=lambda n: (n.start_byte, n.end_byte))\n",
    "        for a,b in zip(top_nodes, top_nodes[1:]):\n",
    "            edges.append(GraphEdge(a.id, b.id, \"SEQ\"))\n",
    "            edges.append(GraphEdge(b.id, a.id, \"SEQ\"))\n",
    "\n",
    "    stack = [tree.root_node]\n",
    "    while stack:\n",
    "        ts_node = stack.pop()\n",
    "        if ts_node.type == \"if_statement\":\n",
    "            if_id = span_to_gid.get((ts_node.start_byte, ts_node.end_byte, ts_node.type))\n",
    "            if if_id is not None:\n",
    "                then_node = ts_node.child_by_field_name(\"consequence\")\n",
    "                else_node = ts_node.child_by_field_name(\"alternative\")\n",
    "\n",
    "                def selected_in_span(s: int, e: int) -> List[int]:\n",
    "                    out = []\n",
    "                    for n in nodes:\n",
    "                        if n.start_byte >= s and n.end_byte <= e:\n",
    "                            out.append(n.id)\n",
    "                    return out\n",
    "\n",
    "                if then_node is not None:\n",
    "                    for tid in selected_in_span(then_node.start_byte, then_node.end_byte):\n",
    "                        if tid == if_id: \n",
    "                            continue\n",
    "                        edges.append(GraphEdge(if_id, tid, \"IF_THEN\"))\n",
    "                        edges.append(GraphEdge(tid, if_id, \"IF_THEN\"))\n",
    "\n",
    "                if else_node is not None:\n",
    "                    for eid in selected_in_span(else_node.start_byte, else_node.end_byte):\n",
    "                        if eid == if_id:\n",
    "                            continue\n",
    "                        edges.append(GraphEdge(if_id, eid, \"IF_ELSE\"))\n",
    "                        edges.append(GraphEdge(eid, if_id, \"IF_ELSE\"))\n",
    "\n",
    "        for ch in reversed(ts_node.children):\n",
    "            if len(ch.children) == 0 and ch.type in (\";\", \"{\", \"}\", \"(\", \")\", \",\"):\n",
    "                continue\n",
    "            stack.append(ch)\n",
    "\n",
    "    return {\n",
    "        \"nodes\": [asdict(n) for n in nodes],\n",
    "        \"edges\": [asdict(e) for e in edges],\n",
    "    }\n",
    "\n",
    "def edges_to_tensors(edges: List[Dict[str,Any]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    src, dst, et = [], [], []\n",
    "    for e in edges:\n",
    "        t = e[\"type\"]\n",
    "        if t not in EDGE_TYPE_TO_ID:\n",
    "            continue\n",
    "        src.append(int(e[\"src\"]))\n",
    "        dst.append(int(e[\"dst\"]))\n",
    "        et.append(int(EDGE_TYPE_TO_ID[t]))\n",
    "    if not src:\n",
    "        return torch.zeros((2,0), dtype=torch.long), torch.zeros((0,), dtype=torch.long)\n",
    "    return torch.tensor([src,dst], dtype=torch.long), torch.tensor(et, dtype=torch.long)\n",
    "\n",
    "# Sanity test\n",
    "demo = \"class Demo{ int foo(int a,int b){ int x=a; if(a>b){x=x+1;} else {x=x-1;} return x; } }\"\n",
    "g = build_method_graph(demo)\n",
    "ei, et = edges_to_tensors(g[\"edges\"])\n",
    "print(\"nodes:\", len(g[\"nodes\"]), \"edges:\", ei.shape[1], \"edge_types:\", sorted({e['type'] for e in g['edges']}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f67d7",
   "metadata": {},
   "source": [
    "## 5) Method extraction from a Java file (Tree-sitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25adb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_methods(java_text: str) -> List[Tuple[str, str]]:\n",
    "    code_bytes = java_text.encode(\"utf-8\")\n",
    "    tree = parser.parse(code_bytes)\n",
    "    root = tree.root_node\n",
    "\n",
    "    out: List[Tuple[str,str]] = []\n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        n = stack.pop()\n",
    "        if n.type in (\"method_declaration\", \"constructor_declaration\"):\n",
    "            name = None\n",
    "            for ch in n.children:\n",
    "                if ch.type == \"identifier\":\n",
    "                    name = code_bytes[ch.start_byte:ch.end_byte].decode(\"utf-8\", errors=\"replace\")\n",
    "                    break\n",
    "            if name is None:\n",
    "                name = n.type\n",
    "            src = code_bytes[n.start_byte:n.end_byte].decode(\"utf-8\", errors=\"replace\")\n",
    "            out.append((name, src))\n",
    "        for ch in reversed(n.children):\n",
    "            stack.append(ch)\n",
    "    return out\n",
    "\n",
    "def java_file_to_methods_jsonl(java_path: Path, out_jsonl: Path) -> int:\n",
    "    text = java_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    methods = extract_methods(text)\n",
    "    if not methods:\n",
    "        methods = [(\"FILE\", text)]\n",
    "    out_jsonl.parent.mkdir(parents=True, exist_ok=True)\n",
    "    n_written = 0\n",
    "    with out_jsonl.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i,(mname, msrc) in enumerate(methods):\n",
    "            g = build_method_graph(msrc)\n",
    "            rec = {\n",
    "                \"method_id\": f\"{java_path}:{mname}:{i}\",\n",
    "                \"method_name\": mname,\n",
    "                \"file\": str(java_path),\n",
    "                \"nodes\": [\n",
    "                    {\n",
    "                        \"id\": n[\"id\"],\n",
    "                        \"kind\": n[\"kind\"],\n",
    "                        \"ast_type\": n[\"ast_type\"],\n",
    "                        \"code\": n[\"code\"],\n",
    "                        \"start_byte\": n[\"start_byte\"],\n",
    "                        \"end_byte\": n[\"end_byte\"],\n",
    "                        \"depth\": n[\"depth\"],\n",
    "                    }\n",
    "                    for n in g[\"nodes\"]\n",
    "                ],\n",
    "                \"edges\": g[\"edges\"],\n",
    "            }\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            n_written += 1\n",
    "    return n_written\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8e19f",
   "metadata": {},
   "source": [
    "## 6) Build per-program artifacts (compile/decompile → methods.jsonl → embed_shard.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ab74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pick_java_for_graph(program_src: Path, artifact_dir: Path) -> Path:\n",
    "    if not USE_DECOMPILED:\n",
    "        return program_src\n",
    "    decomp_dir = artifact_dir / \"decompiled\"\n",
    "    cands = sorted(decomp_dir.rglob(\"*.java\"))\n",
    "    return cands[0] if cands else program_src\n",
    "\n",
    "def embed_methods_jsonl(in_jsonl: Path, out_pt: Path, model, tokenizer, device: str,\n",
    "                        max_length: int = 256, batch_size: int = 32) -> int:\n",
    "    recs = []\n",
    "    parsed = [json.loads(l) for l in in_jsonl.read_text(encoding=\"utf-8\").splitlines() if l.strip()]\n",
    "\n",
    "    for rec in parsed:\n",
    "        nodes = rec[\"nodes\"]\n",
    "        edges = rec[\"edges\"]\n",
    "        texts = [n[\"code\"] for n in nodes]\n",
    "\n",
    "        if not texts:\n",
    "            edge_index, edge_type = edges_to_tensors(edges)\n",
    "            recs.append({\n",
    "                \"method_id\": rec[\"method_id\"],\n",
    "                \"method_name\": rec[\"method_name\"],\n",
    "                \"file\": rec[\"file\"],\n",
    "                \"x\": torch.zeros((0, 768), dtype=torch.float32),\n",
    "                \"edge_index\": edge_index,\n",
    "                \"edge_type\": edge_type,\n",
    "                \"num_nodes\": 0,\n",
    "                \"num_edges\": int(edge_index.shape[1]),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        embs = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            tok = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "            tok = {k:v.to(device) for k,v in tok.items()}\n",
    "            with torch.no_grad():\n",
    "                out = model(**tok)\n",
    "                last = out.last_hidden_state\n",
    "                mask = tok[\"attention_mask\"].unsqueeze(-1)\n",
    "                pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "            embs.append(pooled.detach().cpu())\n",
    "        x = torch.cat(embs, dim=0).to(torch.float32)\n",
    "\n",
    "        edge_index, edge_type = edges_to_tensors(edges)\n",
    "        recs.append({\n",
    "            \"method_id\": rec[\"method_id\"],\n",
    "            \"method_name\": rec[\"method_name\"],\n",
    "            \"file\": rec[\"file\"],\n",
    "            \"x\": x,\n",
    "            \"edge_index\": edge_index,\n",
    "            \"edge_type\": edge_type,\n",
    "            \"num_nodes\": int(x.shape[0]),\n",
    "            \"num_edges\": int(edge_index.shape[1]),\n",
    "        })\n",
    "\n",
    "    out_pt.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(recs, out_pt)\n",
    "    return len(recs)\n",
    "\n",
    "def build_program_artifact(program_src: Path, out_root: Path,\n",
    "                           jdk_home: Path, vineflower_jar: Path,\n",
    "                           model, tokenizer, device: str,\n",
    "                           extra_javac: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    prog_id = sha1(str(program_src))\n",
    "    artifact_dir = out_root / f\"prog_{prog_id}\"\n",
    "\n",
    "    if FORCE_REBUILD and artifact_dir.exists():\n",
    "        shutil.rmtree(artifact_dir)\n",
    "    artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if USE_DECOMPILED:\n",
    "        jar_path = artifact_dir / \"app.jar\"\n",
    "        if not jar_path.exists():\n",
    "            jar_path = compile_single_java(program_src, artifact_dir, jdk_home, extra_javac=extra_javac)\n",
    "        decomp_dir = artifact_dir / \"decompiled\"\n",
    "        if not decomp_dir.exists() or not any(decomp_dir.rglob(\"*.java\")):\n",
    "            decompile_jar(jar_path, decomp_dir, vineflower_jar)\n",
    "\n",
    "    java_for_graph = pick_java_for_graph(program_src, artifact_dir)\n",
    "\n",
    "    methods_jsonl = artifact_dir / \"methods.jsonl\"\n",
    "    if not methods_jsonl.exists():\n",
    "        n_methods = java_file_to_methods_jsonl(java_for_graph, methods_jsonl)\n",
    "    else:\n",
    "        n_methods = sum(1 for _ in methods_jsonl.open(\"r\", encoding=\"utf-8\"))\n",
    "\n",
    "    shard_pt = artifact_dir / \"embed_shard.pt\"\n",
    "    if not shard_pt.exists():\n",
    "        n_rec = embed_methods_jsonl(methods_jsonl, shard_pt, model, tokenizer, device=device,\n",
    "                                    max_length=MAX_LENGTH, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        n_rec = len(torch.load(shard_pt, map_location=\"cpu\"))\n",
    "\n",
    "    return {\n",
    "        \"source_path\": str(program_src),\n",
    "        \"artifact_dir\": str(artifact_dir),\n",
    "        \"java_for_graph\": str(java_for_graph),\n",
    "        \"methods_jsonl\": str(methods_jsonl),\n",
    "        \"embed_shard\": str(shard_pt),\n",
    "        \"num_methods\": int(n_methods),\n",
    "        \"num_method_records\": int(n_rec),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705ec94",
   "metadata": {},
   "source": [
    "## 7) Run artifact build for anchors + clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8617aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1903.99it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "RobertaModel LOAD REPORT from: microsoft/graphcodebert-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.decoder.bias            | UNEXPECTED | \n",
      "lm_head.decoder.weight          | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programs to process: 40\n",
      "[5/40] ok=5 fail=0\n",
      "[10/40] ok=10 fail=0\n",
      "[15/40] ok=15 fail=0\n",
      "[20/40] ok=20 fail=0\n",
      "[25/40] ok=25 fail=0\n",
      "[30/40] ok=30 fail=0\n",
      "[35/40] ok=35 fail=0\n",
      "[40/40] ok=40 fail=0\n",
      "Wrote: /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/pipeline/nb_program_artifacts_type3/program_index.json\n",
      "Done in 0.1s\n",
      "OK: 40 FAILED: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
    "\n",
    "def iter_program_sources(root: Path, clone_type: str, limit_indices: Optional[int]) -> List[Path]:\n",
    "    idxs = list_indices(root)\n",
    "    if limit_indices is not None:\n",
    "        idxs = idxs[:limit_indices]\n",
    "    out: List[Path] = []\n",
    "    for idx in idxs:\n",
    "        a = anchor_path(root, idx)\n",
    "        if a.exists():\n",
    "            out.append(a)\n",
    "        for c in clone_paths(root, idx, clone_type):\n",
    "            if c.exists():\n",
    "                out.append(c)\n",
    "    seen, uniq = set(), []\n",
    "    for p in out:\n",
    "        s = str(p.resolve())\n",
    "        if s not in seen:\n",
    "            seen.add(s); uniq.append(p)\n",
    "    return uniq\n",
    "\n",
    "sources = iter_program_sources(DATASET_ROOT, CLONE_TYPE, LIMIT_INDICES)\n",
    "print(\"Programs to process:\", len(sources))\n",
    "\n",
    "program_index: Dict[str, Any] = {\"items\": {}, \"failures\": []}\n",
    "t0 = time.time()\n",
    "\n",
    "for i, src in enumerate(sources, 1):\n",
    "    try:\n",
    "        item = build_program_artifact(\n",
    "            src, OUT_DIR,\n",
    "            jdk_home=JDK_HOME,\n",
    "            vineflower_jar=VINEFLOWER_JAR,\n",
    "            model=model, tokenizer=tokenizer, device=DEVICE,\n",
    "            extra_javac=None,  # add flags later\n",
    "        )\n",
    "        program_index[\"items\"][str(Path(src).resolve())] = item\n",
    "    except Exception as e:\n",
    "        program_index[\"failures\"].append({\"source_path\": str(src), \"error\": str(e)})\n",
    "    if i % 5 == 0:\n",
    "        print(f\"[{i}/{len(sources)}] ok={len(program_index['items'])} fail={len(program_index['failures'])}\")\n",
    "\n",
    "program_index_path = OUT_DIR / \"program_index.json\"\n",
    "program_index_path.write_text(json.dumps(program_index, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote:\", program_index_path)\n",
    "print(\"Done in %.1fs\" % (time.time()-t0))\n",
    "print(\"OK:\", len(program_index[\"items\"]), \"FAILED:\", len(program_index[\"failures\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f04627",
   "metadata": {},
   "source": [
    "## 8) Program store loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9371169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program: /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/data/code-clone-dataset/dataset/base/01/main.java\n",
      "num_methods: 6\n",
      "x shape: torch.Size([1, 768]) edge_index: torch.Size([2, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_program_methods(program_index: Dict[str,Any], program_src: str) -> Optional[List[Dict[str,Any]]]:\n",
    "    item = program_index[\"items\"].get(str(Path(program_src).resolve()))\n",
    "    if not item:\n",
    "        return None\n",
    "    shard = Path(item[\"embed_shard\"])\n",
    "    if not shard.exists():\n",
    "        return None\n",
    "    return torch.load(shard, map_location=\"cpu\")\n",
    "\n",
    "some_key = next(iter(program_index[\"items\"].keys()))\n",
    "methods = load_program_methods(program_index, some_key)\n",
    "print(\"Example program:\", some_key)\n",
    "print(\"num_methods:\", len(methods))\n",
    "print(\"x shape:\", methods[0][\"x\"].shape, \"edge_index:\", methods[0][\"edge_index\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19a8d5",
   "metadata": {},
   "source": [
    "## 9) Simple GNN encoder + program pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb6c87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scatter_mean(src: torch.Tensor, index: torch.Tensor, dim_size: int) -> torch.Tensor:\n",
    "    H = src.shape[-1]\n",
    "    out = torch.zeros((dim_size, H), device=src.device, dtype=src.dtype)\n",
    "    cnt = torch.zeros((dim_size, 1), device=src.device, dtype=src.dtype)\n",
    "    out.index_add_(0, index, src)\n",
    "    cnt.index_add_(0, index, torch.ones((index.shape[0],1), device=src.device, dtype=src.dtype))\n",
    "    return out / cnt.clamp(min=1.0)\n",
    "\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_dim: int = 768, hidden: int = 256, layers: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(in_dim, hidden)\n",
    "        self.layers = nn.ModuleList([nn.Linear(hidden, hidden) for _ in range(layers)])\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        h = F.relu(self.in_proj(x))\n",
    "        for lin in self.layers:\n",
    "            if edge_index.numel() == 0 or h.shape[0] == 0:\n",
    "                h = F.relu(lin(h))\n",
    "                continue\n",
    "            src, dst = edge_index[0], edge_index[1]\n",
    "            agg = scatter_mean(h[src], dst, dim_size=h.shape[0])\n",
    "            h = h + agg\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            h = F.relu(lin(h))\n",
    "        return h\n",
    "\n",
    "def encode_method(gnn: SimpleGNN, rec: Dict[str,Any], device: str) -> torch.Tensor:\n",
    "    x = rec[\"x\"].to(device)\n",
    "    ei = rec[\"edge_index\"].to(device)\n",
    "    if x.shape[0] == 0:\n",
    "        return torch.zeros((gnn.in_proj.out_features,), device=device)\n",
    "    h = gnn(x, ei)\n",
    "    return h.mean(dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_program_mean(gnn: SimpleGNN, method_recs: List[Dict[str,Any]], device: str) -> torch.Tensor:\n",
    "    if not method_recs:\n",
    "        return torch.zeros((gnn.in_proj.out_features,), device=device)\n",
    "    embs = [encode_method(gnn, rec, device) for rec in method_recs]\n",
    "    return torch.stack(embs, dim=0).mean(dim=0)\n",
    "\n",
    "class PairClassifier(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim * 2, dim)\n",
    "        self.fc2 = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, ha: torch.Tensor, hb: torch.Tensor) -> torch.Tensor:\n",
    "        feat = torch.cat([torch.abs(ha - hb), ha * hb], dim=-1)\n",
    "        z = F.relu(self.fc1(feat))\n",
    "        return self.fc2(z).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87f2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception:\n",
    "    plt = None\n",
    "\n",
    "\n",
    "def _auc_roc_from_scores(y_true: List[int], y_score: List[float]) -> Tuple[float, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Pure-python ROC AUC (no sklearn dependency).\n",
    "    Returns: (auc, fpr_list, tpr_list)\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_score) and len(y_true) > 0\n",
    "\n",
    "    # Count positives/negatives\n",
    "    p = sum(1 for y in y_true if y == 1)\n",
    "    n = len(y_true) - p\n",
    "    if p == 0 or n == 0:\n",
    "        return float(\"nan\"), [], []\n",
    "\n",
    "    # Sort by score descending\n",
    "    pairs = sorted(zip(y_score, y_true), key=lambda t: t[0], reverse=True)\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tpr = [0.0]\n",
    "    fpr = [0.0]\n",
    "\n",
    "    # Walk thresholds (at each unique score)\n",
    "    last_score = None\n",
    "    for score, y in pairs:\n",
    "        if last_score is None:\n",
    "            last_score = score\n",
    "\n",
    "        # If score changed, record point before stepping into next threshold\n",
    "        if score != last_score:\n",
    "            tpr.append(tp / p)\n",
    "            fpr.append(fp / n)\n",
    "            last_score = score\n",
    "\n",
    "        if y == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    # Final point\n",
    "    tpr.append(tp / p)\n",
    "    fpr.append(fp / n)\n",
    "\n",
    "    # Trapezoidal area\n",
    "    auc = 0.0\n",
    "    for i in range(1, len(fpr)):\n",
    "        auc += (fpr[i] - fpr[i - 1]) * (tpr[i] + tpr[i - 1]) / 2.0\n",
    "\n",
    "    return float(auc), fpr, tpr\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_stream_auc(\n",
    "    gnn,\n",
    "    clf,\n",
    "    program_index: Dict[str, Any],\n",
    "    pair_it: Iterator[Tuple[str, str, int]],\n",
    "    device: str,\n",
    "    num_pairs: int,\n",
    "    *,\n",
    "    plot: bool = False,\n",
    "    title: str = \"ROC (val)\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluates `num_pairs` from pair_it.\n",
    "    Returns val_loss, val_acc, auc, and bookkeeping counts.\n",
    "\n",
    "    Uses y_score = sigmoid(logit) for ROC AUC.\n",
    "    \"\"\"\n",
    "    gnn.eval()\n",
    "    clf.eval()\n",
    "\n",
    "    losses: List[torch.Tensor] = []\n",
    "    y_true: List[int] = []\n",
    "    y_score: List[float] = []\n",
    "\n",
    "    used = 0\n",
    "    pos = 0\n",
    "    correct = 0\n",
    "\n",
    "    for _ in range(num_pairs):\n",
    "        a, b, y = next(pair_it)\n",
    "\n",
    "        ma = load_program_methods(program_index, a)\n",
    "        mb = load_program_methods(program_index, b)\n",
    "        if ma is None or mb is None:\n",
    "            continue\n",
    "\n",
    "        ha = encode_program_mean(gnn, ma, device)\n",
    "        hb = encode_program_mean(gnn, mb, device)\n",
    "\n",
    "        logit = clf(ha, hb).view(())  # scalar\n",
    "        target = torch.tensor(float(y), device=device)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(logit, target)\n",
    "        prob = torch.sigmoid(logit).item()\n",
    "\n",
    "        pred = 1 if prob >= 0.5 else 0\n",
    "        correct += int(pred == int(y))\n",
    "\n",
    "        used += 1\n",
    "        pos += int(y == 1)\n",
    "        losses.append(loss.detach().cpu())\n",
    "        y_true.append(int(y))\n",
    "        y_score.append(float(prob))\n",
    "\n",
    "    if used == 0:\n",
    "        return {\n",
    "            \"val_loss\": None,\n",
    "            \"val_acc\": None,\n",
    "            \"auc\": None,\n",
    "            \"used\": 0,\n",
    "            \"pos\": 0,\n",
    "            \"neg\": 0,\n",
    "        }\n",
    "\n",
    "    val_loss = float(torch.stack(losses).mean())\n",
    "    val_acc = correct / used\n",
    "    auc, fpr, tpr = _auc_roc_from_scores(y_true, y_score)\n",
    "\n",
    "    if plot and plt is not None and fpr and tpr and not math.isnan(auc):\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"{title} | AUC={auc:.3f} | used={used} pos={pos} neg={used-pos}\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"auc\": auc if not math.isnan(auc) else None,\n",
    "        \"used\": used,\n",
    "        \"pos\": pos,\n",
    "        \"neg\": used - pos,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8f92e",
   "metadata": {},
   "source": [
    "## 10) Training loop (program-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c5ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainCfg:\n",
    "    steps: int = 2000\n",
    "    batch_pairs: int = 32\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    log_every: int = 50\n",
    "    eval_every: int = 200\n",
    "    val_pairs: int = 200\n",
    "    pos_ratio: float = 0.5\n",
    "    hidden: int = 256\n",
    "    layers: int = 3\n",
    "    dropout: float = 0.1\n",
    "    val_ratio: float = 0.2\n",
    "\n",
    "cfg = TrainCfg()\n",
    "\n",
    "def batch_k(it: Iterator[Tuple[str,str,int]], k: int) -> List[Tuple[str,str,int]]:\n",
    "    return [next(it) for _ in range(k)]\n",
    "\n",
    "def eval_stream(gnn, clf, program_index, pair_it, device: str, num_pairs: int) -> Dict[str, Any]:\n",
    "    gnn.eval(); clf.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    used = 0\n",
    "    pos = 0\n",
    "    for _ in range(num_pairs):\n",
    "        a,b,y = next(pair_it)\n",
    "        ma = load_program_methods(program_index, a)\n",
    "        mb = load_program_methods(program_index, b)\n",
    "        if ma is None or mb is None:\n",
    "            continue\n",
    "        ha = encode_program_mean(gnn, ma, device)\n",
    "        hb = encode_program_mean(gnn, mb, device)\n",
    "        logit = clf(ha, hb)\n",
    "        target = torch.tensor(float(y), device=device)\n",
    "        loss = F.binary_cross_entropy_with_logits(logit, target)\n",
    "        prob = torch.sigmoid(logit).item()\n",
    "        pred = 1 if prob >= 0.5 else 0\n",
    "        correct += int(pred == int(y))\n",
    "        pos += int(y == 1)\n",
    "        used += 1\n",
    "        losses.append(loss.detach().cpu())\n",
    "    if used == 0:\n",
    "        return {\"val_loss\": None, \"val_acc\": None, \"used\": 0, \"pos\": 0, \"neg\": 0}\n",
    "    return {\n",
    "        \"val_loss\": float(torch.stack(losses).mean()),\n",
    "        \"val_acc\": correct / used,\n",
    "        \"used\": used,\n",
    "        \"pos\": pos,\n",
    "        \"neg\": used - pos,\n",
    "    }\n",
    "\n",
    "def train_program_model(program_index: Dict[str,Any],\n",
    "                        train_it: Iterator[Tuple[str,str,int]],\n",
    "                        val_it: Iterator[Tuple[str,str,int]],\n",
    "                        device: str,\n",
    "                        cfg: TrainCfg):\n",
    "    torch.manual_seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    gnn = SimpleGNN(in_dim=768, hidden=cfg.hidden, layers=cfg.layers, dropout=cfg.dropout).to(device)\n",
    "    clf = PairClassifier(dim=cfg.hidden).to(device)\n",
    "    opt = torch.optim.AdamW(list(gnn.parameters()) + list(clf.parameters()), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    hist = []\n",
    "    ema = None\n",
    "    gnn.train(); clf.train()\n",
    "\n",
    "    for step in range(1, cfg.steps + 1):\n",
    "        batch = batch_k(train_it, cfg.batch_pairs)\n",
    "        logits, targets = [], []\n",
    "        for a,b,y in batch:\n",
    "            ma = load_program_methods(program_index, a)\n",
    "            mb = load_program_methods(program_index, b)\n",
    "            if ma is None or mb is None:\n",
    "                continue\n",
    "            ha = encode_program_mean(gnn, ma, device)\n",
    "            hb = encode_program_mean(gnn, mb, device)\n",
    "            logits.append(clf(ha, hb))\n",
    "            targets.append(torch.tensor(float(y), device=device))\n",
    "        if not logits:\n",
    "            continue\n",
    "\n",
    "        logits_t = torch.stack(logits)\n",
    "        targets_t = torch.stack(targets)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits_t, targets_t)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        l = float(loss.detach().cpu())\n",
    "        ema = l if ema is None else 0.98 * ema + 0.02 * l\n",
    "\n",
    "        if step % cfg.log_every == 0:\n",
    "            print(f\"step={step} loss={l:.4f} ema={ema:.4f}\")\n",
    "\n",
    "        if step % cfg.eval_every == 0:\n",
    "            metrics = eval_stream_auc(\n",
    "                gnn, clf, program_index, val_it,\n",
    "                device=device,\n",
    "                num_pairs=cfg.val_pairs,\n",
    "                plot=False,               # set True to draw ROC every eval\n",
    "                title=f\"ROC at step {step}\",\n",
    "            )\n",
    "            print(\n",
    "                f\"[VAL] used={metrics['used']} pos={metrics['pos']} neg={metrics['neg']} \"\n",
    "                f\"val_loss={metrics['val_loss']} val_acc={metrics['val_acc']} auc={metrics['auc']}\"\n",
    "            )\n",
    "            hist.append({\"step\": step, \"train_loss\": l, \"ema\": ema, **metrics})\n",
    "            gnn.train(); clf.train()\n",
    "\n",
    "    return gnn, clf, hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87268dd0",
   "metadata": {},
   "source": [
    "## 11) Train/val split + training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cda4e041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_idxs: 8 val_idxs: 2\n",
      "step=50 loss=0.6179 ema=0.6615\n",
      "step=100 loss=0.5230 ema=0.5926\n",
      "step=150 loss=0.5616 ema=0.5319\n",
      "step=200 loss=0.5326 ema=0.5138\n",
      "[VAL] used=200 pos=99 neg=101 val_loss=0.4904137849807739 val_acc=0.755 auc=0.800980098009801\n",
      "step=250 loss=0.4489 ema=0.5023\n",
      "step=300 loss=0.3997 ema=0.4761\n",
      "step=350 loss=0.5158 ema=0.4654\n",
      "step=400 loss=0.4355 ema=0.4504\n",
      "[VAL] used=200 pos=96 neg=104 val_loss=0.4218173325061798 val_acc=0.78 auc=0.8963341346153846\n",
      "step=450 loss=0.4734 ema=0.4288\n",
      "step=500 loss=0.3303 ema=0.4049\n",
      "step=550 loss=0.4386 ema=0.3932\n",
      "step=600 loss=0.2716 ema=0.3759\n",
      "[VAL] used=200 pos=102 neg=98 val_loss=0.535530149936676 val_acc=0.68 auc=0.8373349339735896\n",
      "step=650 loss=0.4560 ema=0.3612\n",
      "step=700 loss=0.2745 ema=0.3651\n",
      "step=750 loss=0.3722 ema=0.3421\n",
      "step=800 loss=0.3061 ema=0.3128\n",
      "[VAL] used=200 pos=92 neg=108 val_loss=0.5024121999740601 val_acc=0.805 auc=0.8596014492753622\n",
      "step=850 loss=0.2008 ema=0.3068\n",
      "step=900 loss=0.2904 ema=0.3018\n",
      "step=950 loss=0.2488 ema=0.3170\n",
      "step=1000 loss=0.3679 ema=0.3030\n",
      "[VAL] used=200 pos=106 neg=94 val_loss=0.44892075657844543 val_acc=0.805 auc=0.9010437575270975\n",
      "step=1050 loss=0.2673 ema=0.2964\n",
      "step=1100 loss=0.3545 ema=0.2898\n",
      "step=1150 loss=0.2740 ema=0.2890\n",
      "step=1200 loss=0.3435 ema=0.2651\n",
      "[VAL] used=200 pos=107 neg=93 val_loss=0.5261616706848145 val_acc=0.83 auc=0.9026228519746758\n",
      "step=1250 loss=0.2568 ema=0.2670\n",
      "step=1300 loss=0.2044 ema=0.2639\n",
      "step=1350 loss=0.2182 ema=0.2708\n",
      "step=1400 loss=0.2128 ema=0.2776\n",
      "[VAL] used=200 pos=98 neg=102 val_loss=0.36950814723968506 val_acc=0.83 auc=0.9394757903161265\n",
      "step=1450 loss=0.1683 ema=0.2703\n",
      "step=1500 loss=0.2802 ema=0.2747\n",
      "step=1550 loss=0.2759 ema=0.2561\n",
      "step=1600 loss=0.3095 ema=0.2644\n",
      "[VAL] used=200 pos=103 neg=97 val_loss=0.485128790140152 val_acc=0.86 auc=0.9109198278450605\n",
      "step=1650 loss=0.2173 ema=0.2659\n",
      "step=1700 loss=0.2762 ema=0.2648\n",
      "step=1750 loss=0.3202 ema=0.2752\n",
      "step=1800 loss=0.3389 ema=0.2638\n",
      "[VAL] used=200 pos=95 neg=105 val_loss=0.4041043519973755 val_acc=0.825 auc=0.9316290726817043\n",
      "step=1850 loss=0.1326 ema=0.2461\n",
      "step=1900 loss=0.3491 ema=0.2338\n",
      "step=1950 loss=0.3692 ema=0.2601\n",
      "step=2000 loss=0.3157 ema=0.2504\n",
      "[VAL] used=200 pos=98 neg=102 val_loss=0.3720952570438385 val_acc=0.825 auc=0.9411764705882352\n",
      "Saved models to: /Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/pipeline/nb_program_artifacts_type3/models\n",
      "History points: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_idx_from_synthetic_path(p: str) -> Optional[str]:\n",
    "    parts = Path(p).parts\n",
    "    for i, seg in enumerate(parts):\n",
    "        if seg == \"base\" or seg.startswith(\"type-\"):\n",
    "            if i + 1 < len(parts):\n",
    "                return parts[i+1]\n",
    "    return None\n",
    "\n",
    "def split_indices(idxs: List[str], val_ratio: float, seed: int) -> Tuple[set[str], set[str]]:\n",
    "    rng = random.Random(seed)\n",
    "    idxs = idxs[:]\n",
    "    rng.shuffle(idxs)\n",
    "    n_val = max(1, int(round(len(idxs) * val_ratio))) if idxs else 0\n",
    "    val = set(idxs[:n_val])\n",
    "    train = set(idxs[n_val:])\n",
    "    return train, val\n",
    "\n",
    "def filter_pairs_by_anchor_index(pair_it, allowed: set[str]) -> Iterator[Tuple[str,str,int]]:\n",
    "    while True:\n",
    "        a,b,y = next(pair_it)\n",
    "        ia = extract_idx_from_synthetic_path(a)\n",
    "        if ia is None:\n",
    "            continue\n",
    "        if ia in allowed:\n",
    "            yield (a,b,y)\n",
    "\n",
    "all_idxs = list_indices(DATASET_ROOT)\n",
    "if LIMIT_INDICES is not None:\n",
    "    all_idxs = all_idxs[:LIMIT_INDICES]\n",
    "\n",
    "train_idxs, val_idxs = split_indices(all_idxs, val_ratio=cfg.val_ratio, seed=SEED)\n",
    "print(\"train_idxs:\", len(train_idxs), \"val_idxs:\", len(val_idxs))\n",
    "\n",
    "pos = positive_pairs(root=DATASET_ROOT, clone_type=CLONE_TYPE, seed=SEED, limit_indices=LIMIT_INDICES)\n",
    "neg = negative_pairs(root=DATASET_ROOT, clone_type=CLONE_TYPE, seed=SEED, limit_indices=LIMIT_INDICES)\n",
    "mix = interleave(pos, neg, pos_ratio=cfg.pos_ratio, seed=SEED)\n",
    "\n",
    "pos_v = positive_pairs(root=DATASET_ROOT, clone_type=CLONE_TYPE, seed=SEED+999, limit_indices=LIMIT_INDICES)\n",
    "neg_v = negative_pairs(root=DATASET_ROOT, clone_type=CLONE_TYPE, seed=SEED+999, limit_indices=LIMIT_INDICES)\n",
    "mix_v = interleave(pos_v, neg_v, pos_ratio=cfg.pos_ratio, seed=SEED+999)\n",
    "\n",
    "train_it = filter_pairs_by_anchor_index(mix, train_idxs)\n",
    "val_it = filter_pairs_by_anchor_index(mix_v, val_idxs)\n",
    "\n",
    "gnn, clf, hist = train_program_model(program_index, train_it, val_it, device=DEVICE, cfg=cfg)\n",
    "\n",
    "model_dir = OUT_DIR / \"models\"\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(gnn.state_dict(), model_dir / \"gnn.pt\")\n",
    "torch.save(clf.state_dict(), model_dir / \"clf.pt\")\n",
    "(model_dir / \"history.json\").write_text(json.dumps(hist, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved models to:\", model_dir)\n",
    "print(\"History points:\", len(hist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f01c4",
   "metadata": {},
   "source": [
    "## 12) Inspect last validation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b1b046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step': 1200, 'train_loss': 0.3435484766960144, 'ema': 0.2651319465204026, 'val_loss': 0.5261616706848145, 'val_acc': 0.83, 'auc': 0.9026228519746758, 'used': 200, 'pos': 107, 'neg': 93}\n",
      "{'step': 1400, 'train_loss': 0.212834894657135, 'ema': 0.27757663397911636, 'val_loss': 0.36950814723968506, 'val_acc': 0.83, 'auc': 0.9394757903161265, 'used': 200, 'pos': 98, 'neg': 102}\n",
      "{'step': 1600, 'train_loss': 0.3095071315765381, 'ema': 0.26437214597773917, 'val_loss': 0.485128790140152, 'val_acc': 0.86, 'auc': 0.9109198278450605, 'used': 200, 'pos': 103, 'neg': 97}\n",
      "{'step': 1800, 'train_loss': 0.33893224596977234, 'ema': 0.2638259846502894, 'val_loss': 0.4041043519973755, 'val_acc': 0.825, 'auc': 0.9316290726817043, 'used': 200, 'pos': 95, 'neg': 105}\n",
      "{'step': 2000, 'train_loss': 0.31572890281677246, 'ema': 0.2504228056128155, 'val_loss': 0.3720952570438385, 'val_acc': 0.825, 'auc': 0.9411764705882352, 'used': 200, 'pos': 98, 'neg': 102}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if hist:\n",
    "    for r in hist[-5:]:\n",
    "        print(r)\n",
    "else:\n",
    "    print(\"No eval points recorded yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f46b678c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.43469318747520447,\n",
       " 'val_acc': 0.81,\n",
       " 'auc': 0.9210457367317876,\n",
       " 'used': 2000,\n",
       " 'pos': 996,\n",
       " 'neg': 1004}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training: fresh validation stream for final ROC/AUC\n",
    "pos_v2 = positive_pairs(root=DATASET_ROOT, clone_type=CLONE_TYPE, seed=SEED+999, limit_indices=LIMIT_INDICES)\n",
    "neg_v2 = negative_pairs(root=DATASET_ROOT, clone_type=CLONE_TYPE, seed=SEED+999, limit_indices=LIMIT_INDICES)\n",
    "mix_v2 = interleave(pos_v2, neg_v2, pos_ratio=cfg.pos_ratio, seed=SEED+999)\n",
    "val_it2 = filter_pairs_by_anchor_index(mix_v2, val_idxs)\n",
    "\n",
    "final = eval_stream_auc(gnn, clf, program_index, val_it2, device=DEVICE, num_pairs=2000, plot=True, title=\"Final ROC (val)\")\n",
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
