{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b9a2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.signal import savgol_filter\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "from transformers import pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import javalang\n",
    "import tree_sitter_java\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from data.dataset_factory import get_dataset_generator\n",
    "from data.data_generators.sourcecodeplag_dataset_gen import original_plag_triplet_generator\n",
    "from preprocessing.embedding_chunks import get_ready_to_embed_chunks\n",
    "from preprocessing.context_chunker import safe_get_ready_to_embed_context_chunks\n",
    "from preprocessing.mean_pool_chunks import mean_pool_chunks\n",
    "from preprocessing.block_splitter import deverbose_ast\n",
    "from visualizer.smoothing import smooth_embeddings, smooth_multiple_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04938dd5",
   "metadata": {},
   "source": [
    "# Tree Sitter for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b69ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_java\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "JAVA_LANGUAGE = Language(tree_sitter_java.language())\n",
    "parser = Parser(JAVA_LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7556d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1349.06it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "RobertaModel LOAD REPORT from: microsoft/graphcodebert-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.decoder.weight          | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.decoder.bias            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/graphcodebert-base\"\n",
    "pipe = pipeline(\"feature-extraction\", model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f699e2",
   "metadata": {},
   "source": [
    "# Data and representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4de1bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkKind(Enum):\n",
    "    STRAIGHT = \"straight\"\n",
    "    CONTROL = \"control\"\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    text: str\n",
    "    embedding: None\n",
    "    ast_depth: int\n",
    "    kind: ChunkKind\n",
    "\n",
    "\n",
    "CONTROL_NODES = (\n",
    "    \"if_statement\",\n",
    "    \"for_statement\",\n",
    "    \"while_statement\",\n",
    "    \"do_statement\",\n",
    "    \"switch_statement\",\n",
    "    \"try_statement\",\n",
    "    \"catch_clause\",\n",
    ")\n",
    "\n",
    "STRAIGHT_NODES = (\n",
    "    \"local_variable_declaration\",\n",
    "    \"expression_statement\",\n",
    "    \"return_statement\",\n",
    "    \"throw_statement\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1a38a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a0e1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_control_header(code: bytes, node):\n",
    "    \"\"\"\n",
    "    Extracts only the control header (condition / signature),\n",
    "    excluding the body.\n",
    "    \"\"\"\n",
    "    # Common Tree-sitter pattern: condition is a child\n",
    "    for child in node.children:\n",
    "        if child.type in (\"condition\", \"parenthesized_expression\"):\n",
    "            return code[child.start_byte:child.end_byte].decode(\"utf8\")\n",
    "\n",
    "    # Fallback: first line only\n",
    "    text = code[node.start_byte:node.end_byte].decode(\"utf8\")\n",
    "    return text.split(\"{\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6158b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JavaChunkExtractor:\n",
    "    def __init__(self, source_code):\n",
    "        self.source = source_code\n",
    "        self.lines = source_code.splitlines()\n",
    "        self.chunks = []\n",
    "\n",
    "    def visit(self, node, depth=0):\n",
    "        if isinstance(node, CONTROL_NODES):\n",
    "            self._add_chunk(node, depth, ChunkKind.CONTROL)\n",
    "\n",
    "        elif isinstance(node, STRAIGHT_NODES):\n",
    "            self._add_chunk(node, depth, ChunkKind.STRAIGHT)\n",
    "\n",
    "        for _, child in node.filter(javalang.tree.Node):\n",
    "            self.visit(child, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e3f2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeSitterJavaChunker:\n",
    "    def __init__(self, source_code: str):\n",
    "        self.code = source_code.encode(\"utf8\")\n",
    "        self.chunks = []\n",
    "\n",
    "    def extract(self):\n",
    "        tree = parser.parse(self.code)\n",
    "        self._visit(tree.root_node, depth=0)\n",
    "        return self.chunks\n",
    "\n",
    "    def _visit(self, node, depth):\n",
    "        if node.type in CONTROL_NODES:\n",
    "            self._add_control_chunk(node, depth)\n",
    "\n",
    "        elif node.type in STRAIGHT_NODES:\n",
    "            self._add_statement_chunk(node, depth)\n",
    "\n",
    "        for child in node.children:\n",
    "            if len(child.children) == 0 and child.type in (';', '{', '}', '(', ')', ','):\n",
    "                continue\n",
    "            self._visit(child, depth + 1)\n",
    "\n",
    "    def _add_statement_chunk(self, node, depth):\n",
    "        text = self.code[node.start_byte:node.end_byte].decode(\"utf8\").strip()\n",
    "        if not text:\n",
    "            return\n",
    "\n",
    "        self.chunks.append(\n",
    "            Chunk(\n",
    "                text=text,\n",
    "                embedding=None,\n",
    "                ast_depth=depth,\n",
    "                kind=ChunkKind.STRAIGHT\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _add_control_chunk(self, node, depth):\n",
    "        text = extract_control_header(self.code, node)\n",
    "        if not text:\n",
    "            return\n",
    "\n",
    "        self.chunks.append(\n",
    "            Chunk(\n",
    "                text=text,\n",
    "                embedding=None,\n",
    "                ast_depth=depth,\n",
    "                kind=ChunkKind.CONTROL\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def control_weight(depth, cap=8):\n",
    "    w = 1.0 + torch.log(torch.tensor(depth + 1.0))\n",
    "\n",
    "    return w\n",
    "    #return min(np.log(depth + 1), cap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0228a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_chunk_java(code: str):\n",
    "    chunker = TreeSitterJavaChunker(code)\n",
    "    chunks = chunker.extract()\n",
    "\n",
    "    S_chunks = [c for c in chunks if c.kind == ChunkKind.STRAIGHT]\n",
    "    C_chunks = [c for c in chunks if c.kind == ChunkKind.CONTROL]\n",
    "\n",
    "    return S_chunks, C_chunks\n",
    "\n",
    "\n",
    "def embed_chunk_text(text):\n",
    "    \"\"\"Embed a single code chunk as a fixed vector\"\"\"\n",
    "    outputs = pipe(text)            # shape: [1, seq_len, hidden_dim]\n",
    "    arr = np.array(outputs[0])      # shape: [seq_len, hidden_dim]\n",
    "    return arr.mean(axis=0)         # mean over tokens → shape: (hidden_dim,)\n",
    "\n",
    "\n",
    "def embed_chunks(S, C):\n",
    "    for chunk in S + C:\n",
    "        text = chunk.text.strip()\n",
    "        if text == \"\":\n",
    "            continue\n",
    "        chunk.embedding = embed_chunk_text(text)\n",
    "\n",
    "\n",
    "def combine_chunks_torch(chunks, device=\"cpu\"):\n",
    "    if len(chunks) == 0:\n",
    "        return torch.zeros(768, device=device)\n",
    "\n",
    "    vectors, weights = [], []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        vec = torch.tensor(chunk.embedding, dtype=torch.float32, device=device)\n",
    "        vectors.append(vec)\n",
    "\n",
    "\n",
    "        # BIAS mot kontroll blokker\n",
    "        if chunk.kind == ChunkKind.CONTROL:\n",
    "            w = control_weight(chunk.ast_depth)\n",
    "        else:\n",
    "            w = 1.0\n",
    "\n",
    "        weights.append(w)\n",
    "\n",
    "    V = torch.stack(vectors)\n",
    "    W = torch.tensor(weights, device=device).unsqueeze(1)\n",
    "\n",
    "    return (V * W).sum(dim=0) / W.sum()\n",
    "\n",
    "\n",
    "\n",
    "def cosine_sim(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Differentiable cosine similarity between two vectors.\n",
    "    Returns a scalar tensor.\n",
    "    \"\"\"\n",
    "    return torch.dot(a, b) / (a.norm() * b.norm() + eps)\n",
    "\n",
    "\n",
    "def triplet_loss(sim_pos: torch.Tensor,\n",
    "                 sim_neg: torch.Tensor,\n",
    "                 margin: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Margin-based triplet loss on cosine similarities.\n",
    "    Encourages sim_pos >= sim_neg + margin.\n",
    "    \"\"\"\n",
    "    return torch.relu(margin + sim_neg - sim_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "541500d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(sim_pos, sim_neg, margin=0.5):\n",
    "    \"\"\"\n",
    "    Cosine-based contrastive loss for triplets.\n",
    "    \"\"\"\n",
    "    pos_loss = (1.0 - sim_pos).pow(2)\n",
    "    neg_loss = torch.relu(sim_neg - margin).pow(2)\n",
    "    return pos_loss + neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5548b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = torch.nn.Parameter(torch.tensor(0.5))  # structure vs content\n",
    "optimizer = torch.optim.Adam([alpha], lr=0.02)\n",
    "\n",
    "#triplets = list(original_plag_triplet_generator(seed=42))\n",
    "\n",
    "patience = 5\n",
    "min_delta = 1e-4\n",
    "best_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "best_alpha = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30b7fba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor triplet in tqdm(triplets):\\n    for key in [\"anchor\", \"clone\", \"nonclone\"]:\\n        code = triplet[key]\\n\\n        if code in embedding_cache:\\n            continue\\n\\n        S, C = parse_and_chunk_java(code)\\n        embed_chunks(S, C)\\n\\n        embedding_cache[code] = {\\n            \"S\": combine_chunks_torch(S).detach(),\\n            \"C\": combine_chunks_torch(C).detach(),\\n        }\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "embedding_cache = {}\n",
    "\n",
    "\"\"\"\n",
    "for triplet in tqdm(triplets):\n",
    "    for key in [\"anchor\", \"clone\", \"nonclone\"]:\n",
    "        code = triplet[key]\n",
    "\n",
    "        if code in embedding_cache:\n",
    "            continue\n",
    "\n",
    "        S, C = parse_and_chunk_java(code)\n",
    "        embed_chunks(S, C)\n",
    "\n",
    "        embedding_cache[code] = {\n",
    "            \"S\": combine_chunks_torch(S).detach(),\n",
    "            \"C\": combine_chunks_torch(C).detach(),\n",
    "        }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbdab34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mini-epoch 1:   0%|          | 0/500 [00:00<?, ?it/s]/var/folders/nn/mj23tg7x6k18vhs8rv7k5ld80000gn/T/ipykernel_96979/2399982807.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vec = torch.tensor(chunk.embedding, dtype=torch.float64, device=device)  # <- float64\n",
      "Mini-epoch 1: 100%|██████████| 500/500 [04:39<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-epoch 1 - Avg Loss: 0.1231 | wS=0.500 wC=0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mini-epoch 2: 100%|██████████| 500/500 [03:15<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-epoch 2 - Avg Loss: 0.1220 | wS=0.500 wC=0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mini-epoch 3: 100%|██████████| 500/500 [03:03<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-epoch 3 - Avg Loss: 0.1240 | wS=0.500 wC=0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mini-epoch 4: 100%|██████████| 500/500 [02:02<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-epoch 4 - Avg Loss: 0.1270 | wS=0.500 wC=0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mini-epoch 5: 100%|██████████| 500/500 [01:44<00:00,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-epoch 5 - Avg Loss: 0.1325 | wS=0.500 wC=0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Cache dictionary: key = chunk text, value = embedding tensor\n",
    "embedding_cache = {}\n",
    "\n",
    "def embed_chunk_text_cached(text):\n",
    "    text = text.strip()\n",
    "    if text == \"\":\n",
    "        return torch.zeros(768, dtype=torch.float32)\n",
    "\n",
    "    if text in embedding_cache:\n",
    "        return embedding_cache[text]\n",
    "\n",
    "\n",
    "    outputs = pipe(text, truncation=True, max_length=512)\n",
    "    arr = np.array(outputs[0])        # shape: [seq_len, hidden_dim]\n",
    "    vec = torch.tensor(arr.mean(axis=0), dtype=torch.float32)  # mean pooling\n",
    "    embedding_cache[text] = vec\n",
    "    return vec\n",
    "\n",
    "\n",
    "def embed_chunk_texts_batched(texts, batch_size=16):\n",
    "    \"\"\"\n",
    "    Embed a list of chunk texts, using cache + batching.\n",
    "    Returns a dict {text: embedding_tensor}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Filter out cached and empty\n",
    "    to_embed = [t for t in texts if t.strip() and t not in embedding_cache]\n",
    "\n",
    "    for i in range(0, len(to_embed), batch_size):\n",
    "        batch = to_embed[i:i + batch_size]\n",
    "        outputs = pipe(batch, truncation=True, max_length=512)\n",
    "\n",
    "        for text, out in zip(batch, outputs):\n",
    "            arr = np.array(out)               # [seq_len, hidden_dim]\n",
    "            vec = torch.tensor(\n",
    "                arr.mean(axis=0),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            embedding_cache[text] = vec\n",
    "            results[text] = vec\n",
    "\n",
    "    # Add cached ones\n",
    "    for t in texts:\n",
    "        if t.strip():\n",
    "            results[t] = embedding_cache[t]\n",
    "        else:\n",
    "            results[t] = torch.zeros(768, dtype=torch.float32)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def embed_chunks_cached_batched(S, C, batch_size=16):\n",
    "    texts = [c.text for c in S + C]\n",
    "    embeddings = embed_chunk_texts_batched(texts, batch_size=batch_size)\n",
    "\n",
    "    for chunk in S + C:\n",
    "        emb = embeddings[chunk.text]\n",
    "\n",
    "        # SAFETY: force shape [768]\n",
    "        if emb.ndim == 2:\n",
    "            emb = emb.mean(dim=0)\n",
    "\n",
    "        chunk.embedding = emb\n",
    "\n",
    "\n",
    "\n",
    "def embed_chunks_cached(S, C):\n",
    "    for chunk in S + C:\n",
    "        if chunk.text.strip() == \"\":\n",
    "            continue\n",
    "        chunk.embedding = embed_chunk_text_cached(chunk.text)\n",
    "\n",
    "\n",
    "# --- Helpers ---\n",
    "def combine_chunks_torch(chunks, device=\"cpu\"):\n",
    "    if len(chunks) == 0:\n",
    "        return torch.zeros(768, dtype=torch.float64, device=device)\n",
    "\n",
    "    vectors, weights = [], []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        \n",
    "\n",
    "        vec = torch.tensor(chunk.embedding, dtype=torch.float64, device=device)  # <- float64\n",
    "\n",
    "        assert vec.ndim == 1 and vec.shape[0] == 768, f\"Bad embedding shape: {vec.shape}\"\n",
    "\n",
    "        vectors.append(vec)\n",
    "\n",
    "        if chunk.kind == ChunkKind.CONTROL:\n",
    "            w = np.log(chunk.ast_depth + 2)\n",
    "        else:\n",
    "            w = 1.0\n",
    "        weights.append(w)\n",
    "\n",
    "    V = torch.stack(vectors)\n",
    "    W = torch.tensor(weights, dtype=torch.float64, device=device).unsqueeze(1)\n",
    "\n",
    "    return (V * W).sum(dim=0) / W.sum()\n",
    "\n",
    "\n",
    "def cosine_sim(a: torch.Tensor, b: torch.Tensor, eps=1e-8):\n",
    "    # Convert both to float32\n",
    "    a = a.float()\n",
    "    b = b.float()\n",
    "    return torch.dot(a, b) / (a.norm() * b.norm() + eps)\n",
    "\n",
    "\n",
    "def contrastive_pair_loss(sim, label, margin=0.5):\n",
    "    \"\"\"\n",
    "    Contrastive loss for pairwise data\n",
    "    sim: cosine similarity ∈ [-1,1]\n",
    "    label: 1=clone, 0=non-clone\n",
    "    \"\"\"\n",
    "    # want sim high if label=1, low if label=0\n",
    "    pos_loss = (1 - sim)**2 * label\n",
    "    neg_loss = F.relu(sim - margin)**2 * (1 - label)\n",
    "    return pos_loss + neg_loss\n",
    "\n",
    "# --- Learnable weight ---\n",
    "alpha = torch.nn.Parameter(torch.tensor(0.5))  # gate between statements vs control\n",
    "optimizer = torch.optim.Adam([alpha], lr=0.02)\n",
    "\n",
    "# --- Dataset generator ---\n",
    "from datasets import load_dataset\n",
    "from data.data_generators.schema import CodeSample\n",
    "\n",
    "def bigclonebench_generator(split=\"train\"):\n",
    "    ds = load_dataset(\"google/code_x_glue_cc_clone_detection_big_clone_bench\", split=split)\n",
    "    for sample in ds:\n",
    "        label = 1 if int(sample[\"label\"]) == 0 else 0  # 1=clone, 0=non-clone\n",
    "        yield CodeSample(code_a=sample[\"func1\"], code_b=sample[\"func2\"], label=label, dataset=\"bigclonebench\")\n",
    "\n",
    "# --- Training loop ---\n",
    "num_epochs = 30\n",
    "best_loss = float(\"inf\")\n",
    "patience = 5\n",
    "stop_counter = 0\n",
    "\n",
    "mini_epoch_size = 2000     # start small (10k–50k)\n",
    "batch_size = 16\n",
    "num_epochs = 30\n",
    "\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Parameters ---\n",
    "mini_epoch_size = 500  # number of samples per mini-epoch\n",
    "batch_size = 16          # for embedding batching\n",
    "num_mini_epochs = 5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "alpha = torch.nn.Parameter(torch.tensor([0.5, 0.5], dtype=torch.float32, device=device))\n",
    "optimizer = torch.optim.Adam([alpha], lr=0.02)\n",
    "lambda_reg = 0.1\n",
    "margin = 0.5\n",
    "\n",
    "# --- Multi-channel loss function ---\n",
    "def multi_channel_loss(f_s, f_c, sim_label):\n",
    "    w_s, w_c = torch.sigmoid(alpha)\n",
    "    f_combined = w_s * f_s + w_c * f_c\n",
    "    sim = torch.dot(f_combined, f_combined) / (f_combined.norm()**2 + 1e-8)\n",
    "\n",
    "    pos_loss = ((1 - sim) ** 2) * sim_label\n",
    "    neg_loss = F.relu(sim - margin) ** 2 * (1 - sim_label)\n",
    "    base_loss = pos_loss + neg_loss\n",
    "\n",
    "    reg_loss = lambda_reg * ((w_s - 0.5)**2 + (w_c - 0.5)**2)\n",
    "    return base_loss + reg_loss, w_s.item(), w_c.item()\n",
    "\n",
    "\n",
    "# --- Prepare dataset ---\n",
    "all_samples = list(bigclonebench_generator(split=\"train\"))\n",
    "\n",
    "# --- Training loop with mini-epochs ---\n",
    "for mini_epoch in range(num_mini_epochs):\n",
    "    mini_samples = random.sample(all_samples, mini_epoch_size)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for sample in tqdm(mini_samples, desc=f\"Mini-epoch {mini_epoch+1}\"):\n",
    "        # Parse & chunk\n",
    "        S_a, C_a = parse_and_chunk_java(sample.code_a)\n",
    "        S_b, C_b = parse_and_chunk_java(sample.code_b)\n",
    "\n",
    "        # Embed (use caching and batching for speed)\n",
    "        embed_chunks_cached(S_a, C_a)\n",
    "        embed_chunks_cached(S_b, C_b)\n",
    "\n",
    "        # Aggregate embeddings\n",
    "        f_s = combine_chunks_torch(S_a, device=device)\n",
    "        f_c = combine_chunks_torch(C_a, device=device)\n",
    "        f_s_b = combine_chunks_torch(S_b, device=device)\n",
    "        f_c_b = combine_chunks_torch(C_b, device=device)\n",
    "\n",
    "        # Combine statement/control embeddings across the pair\n",
    "        f_s_pair = f_s + f_s_b\n",
    "        f_c_pair = f_c + f_c_b\n",
    "\n",
    "        sim_label = torch.tensor(sample.label, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Compute loss with multi-channel regularization\n",
    "        loss, w_s_val, w_c_val = multi_channel_loss(f_s_pair, f_c_pair, sim_label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / mini_epoch_size\n",
    "    print(f\"Mini-epoch {mini_epoch+1} - Avg Loss: {avg_loss:.4f} | wS={w_s_val:.3f} wC={w_c_val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d8c9b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
