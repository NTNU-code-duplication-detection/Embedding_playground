{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9a2e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/Documents/NTNU/Bachelor/code-model-embeddings/venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.signal import savgol_filter\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "from transformers import pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import javalang\n",
    "import tree_sitter_java\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from data.dataset_factory import get_dataset_generator\n",
    "from data.data_generators.sourcecodeplag_dataset_gen import original_plag_triplet_generator\n",
    "from preprocessing.embedding_chunks import get_ready_to_embed_chunks\n",
    "from preprocessing.context_chunker import safe_get_ready_to_embed_context_chunks\n",
    "from preprocessing.mean_pool_chunks import mean_pool_chunks\n",
    "from preprocessing.block_splitter import deverbose_ast\n",
    "from visualizer.smoothing import smooth_embeddings, smooth_multiple_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04938dd5",
   "metadata": {},
   "source": [
    "# Tree Sitter for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b69ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_java\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "JAVA_LANGUAGE = Language(tree_sitter_java.language())\n",
    "parser = Parser(JAVA_LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7556d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1837.93it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "RobertaModel LOAD REPORT from: microsoft/graphcodebert-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.decoder.weight          | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.decoder.bias            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/graphcodebert-base\"\n",
    "pipe = pipeline(\"feature-extraction\", model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f699e2",
   "metadata": {},
   "source": [
    "# Data and representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de1bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkKind(Enum):\n",
    "    STRAIGHT = \"straight\"\n",
    "    CONTROL = \"control\"\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    text: str\n",
    "    embedding: None\n",
    "    ast_depth: int\n",
    "    kind: ChunkKind\n",
    "\n",
    "\n",
    "CONTROL_NODES = (\n",
    "    \"if_statement\",\n",
    "    \"for_statement\",\n",
    "    \"while_statement\",\n",
    "    \"do_statement\",\n",
    "    \"switch_statement\",\n",
    "    \"try_statement\",\n",
    "    \"catch_clause\",\n",
    ")\n",
    "\n",
    "STRAIGHT_NODES = (\n",
    "    \"local_variable_declaration\",\n",
    "    \"expression_statement\",\n",
    "    \"return_statement\",\n",
    "    \"throw_statement\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1a38a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a0e1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_control_header(code: bytes, node):\n",
    "    \"\"\"\n",
    "    Extracts only the control header (condition / signature),\n",
    "    excluding the body.\n",
    "    \"\"\"\n",
    "    # Common Tree-sitter pattern: condition is a child\n",
    "    for child in node.children:\n",
    "        if child.type in (\"condition\", \"parenthesized_expression\"):\n",
    "            return code[child.start_byte:child.end_byte].decode(\"utf8\")\n",
    "\n",
    "    # Fallback: first line only\n",
    "    text = code[node.start_byte:node.end_byte].decode(\"utf8\")\n",
    "    return text.split(\"{\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6158b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JavaChunkExtractor:\n",
    "    def __init__(self, source_code):\n",
    "        self.source = source_code\n",
    "        self.lines = source_code.splitlines()\n",
    "        self.chunks = []\n",
    "\n",
    "    def visit(self, node, depth=0):\n",
    "        if isinstance(node, CONTROL_NODES):\n",
    "            self._add_chunk(node, depth, ChunkKind.CONTROL)\n",
    "\n",
    "        elif isinstance(node, STRAIGHT_NODES):\n",
    "            self._add_chunk(node, depth, ChunkKind.STRAIGHT)\n",
    "\n",
    "        for _, child in node.filter(javalang.tree.Node):\n",
    "            self.visit(child, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e3f2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeSitterJavaChunker:\n",
    "    def __init__(self, source_code: str):\n",
    "        self.code = source_code.encode(\"utf8\")\n",
    "        self.chunks = []\n",
    "\n",
    "    def extract(self):\n",
    "        tree = parser.parse(self.code)\n",
    "        self._visit(tree.root_node, depth=0)\n",
    "        return self.chunks\n",
    "\n",
    "    def _visit(self, node, depth):\n",
    "        if node.type in CONTROL_NODES:\n",
    "            self._add_control_chunk(node, depth)\n",
    "\n",
    "        elif node.type in STRAIGHT_NODES:\n",
    "            self._add_statement_chunk(node, depth)\n",
    "\n",
    "        for child in node.children:\n",
    "            if len(child.children) == 0 and child.type in (';', '{', '}', '(', ')', ','):\n",
    "                continue\n",
    "            self._visit(child, depth + 1)\n",
    "\n",
    "    def _add_statement_chunk(self, node, depth):\n",
    "        text = self.code[node.start_byte:node.end_byte].decode(\"utf8\").strip()\n",
    "        if not text:\n",
    "            return\n",
    "\n",
    "        self.chunks.append(\n",
    "            Chunk(\n",
    "                text=text,\n",
    "                embedding=None,\n",
    "                ast_depth=depth,\n",
    "                kind=ChunkKind.STRAIGHT\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _add_control_chunk(self, node, depth):\n",
    "        text = extract_control_header(self.code, node)\n",
    "        if not text:\n",
    "            return\n",
    "\n",
    "        self.chunks.append(\n",
    "            Chunk(\n",
    "                text=text,\n",
    "                embedding=None,\n",
    "                ast_depth=depth,\n",
    "                kind=ChunkKind.CONTROL\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def control_weight(depth, cap=8):\n",
    "    w = 1.0 + torch.log(torch.tensor(depth + 1.0))\n",
    "\n",
    "    return w\n",
    "    #return min(np.log(depth + 1), cap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0228a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_chunk_java(code: str):\n",
    "    chunker = TreeSitterJavaChunker(code)\n",
    "    chunks = chunker.extract()\n",
    "\n",
    "    S_chunks = [c for c in chunks if c.kind == ChunkKind.STRAIGHT]\n",
    "    C_chunks = [c for c in chunks if c.kind == ChunkKind.CONTROL]\n",
    "\n",
    "    return S_chunks, C_chunks\n",
    "\n",
    "\n",
    "def embed_chunk_text(text):\n",
    "    \"\"\"Embed a single code chunk as a fixed vector\"\"\"\n",
    "    outputs = pipe(text)            # shape: [1, seq_len, hidden_dim]\n",
    "    arr = np.array(outputs[0])      # shape: [seq_len, hidden_dim]\n",
    "    return arr.mean(axis=0)         # mean over tokens → shape: (hidden_dim,)\n",
    "\n",
    "\n",
    "def embed_chunks(S, C):\n",
    "    for chunk in S + C:\n",
    "        text = chunk.text.strip()\n",
    "        if text == \"\":\n",
    "            continue\n",
    "        chunk.embedding = embed_chunk_text(text)\n",
    "\n",
    "\n",
    "def combine_chunks_torch(chunks, device=\"cpu\"):\n",
    "    if len(chunks) == 0:\n",
    "        return torch.zeros(768, device=device)\n",
    "\n",
    "    vectors, weights = [], []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        vec = torch.tensor(chunk.embedding, dtype=torch.float32, device=device)\n",
    "        vectors.append(vec)\n",
    "\n",
    "\n",
    "        # BIAS mot kontroll blokker\n",
    "        if chunk.kind == ChunkKind.CONTROL:\n",
    "            w = control_weight(chunk.ast_depth)\n",
    "        else:\n",
    "            w = 1.0\n",
    "\n",
    "        weights.append(w)\n",
    "\n",
    "    V = torch.stack(vectors)\n",
    "    W = torch.tensor(weights, device=device).unsqueeze(1)\n",
    "\n",
    "    return (V * W).sum(dim=0) / W.sum()\n",
    "\n",
    "\n",
    "\n",
    "def cosine_sim(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Differentiable cosine similarity between two vectors.\n",
    "    Returns a scalar tensor.\n",
    "    \"\"\"\n",
    "    return torch.dot(a, b) / (a.norm() * b.norm() + eps)\n",
    "\n",
    "\n",
    "def triplet_loss(sim_pos: torch.Tensor,\n",
    "                 sim_neg: torch.Tensor,\n",
    "                 margin: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Margin-based triplet loss on cosine similarities.\n",
    "    Encourages sim_pos >= sim_neg + margin.\n",
    "    \"\"\"\n",
    "    return torch.relu(margin + sim_neg - sim_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541500d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(sim_pos, sim_neg, margin=0.5):\n",
    "    \"\"\"\n",
    "    Cosine-based contrastive loss for triplets.\n",
    "    \"\"\"\n",
    "    pos_loss = (1.0 - sim_pos).pow(2)\n",
    "    neg_loss = torch.relu(sim_neg - margin).pow(2)\n",
    "    return pos_loss + neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5548b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = torch.nn.Parameter(torch.tensor(0.5))  # structure vs content\n",
    "optimizer = torch.optim.Adam([alpha], lr=0.02)\n",
    "\n",
    "triplets = list(original_plag_triplet_generator(seed=42))\n",
    "\n",
    "patience = 5\n",
    "min_delta = 1e-4\n",
    "best_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "best_alpha = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b7fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 355/355 [01:00<00:00,  5.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "embedding_cache = {}\n",
    "\n",
    "for triplet in tqdm(triplets):\n",
    "    for key in [\"anchor\", \"clone\", \"nonclone\"]:\n",
    "        code = triplet[key]\n",
    "\n",
    "        if code in embedding_cache:\n",
    "            continue\n",
    "\n",
    "        S, C = parse_and_chunk_java(code)\n",
    "        embed_chunks(S, C)\n",
    "\n",
    "        embedding_cache[code] = {\n",
    "            \"S\": combine_chunks_torch(S).detach(),\n",
    "            \"C\": combine_chunks_torch(C).detach(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbdab34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.0736, wS=0.038, wC=0.962\n",
      "Epoch 1: loss=0.0659, wS=0.018, wC=0.982\n",
      "Epoch 2: loss=0.0652, wS=0.011, wC=0.989\n",
      "Epoch 3: loss=0.0649, wS=0.007, wC=0.993\n",
      "Epoch 4: loss=0.0648, wS=0.005, wC=0.995\n",
      "Epoch 5: loss=0.0647, wS=0.004, wC=0.996\n",
      "Epoch 6: loss=0.0646, wS=0.003, wC=0.997\n",
      "Epoch 7: loss=0.0646, wS=0.002, wC=0.998\n",
      "Epoch 8: loss=0.0646, wS=0.002, wC=0.998\n",
      "Epoch 9: loss=0.0646, wS=0.001, wC=0.999\n",
      "Epoch 10: loss=0.0646, wS=0.001, wC=0.999\n",
      "Epoch 11: loss=0.0645, wS=0.001, wC=0.999\n",
      "Early stopping at epoch 11. Best loss=0.0646\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for triplet in triplets:\n",
    "        A_S = embedding_cache[triplet[\"anchor\"]][\"S\"]\n",
    "        A_C = embedding_cache[triplet[\"anchor\"]][\"C\"]\n",
    "\n",
    "        P_S = embedding_cache[triplet[\"clone\"]][\"S\"]\n",
    "        P_C = embedding_cache[triplet[\"clone\"]][\"C\"]\n",
    "\n",
    "        N_S = embedding_cache[triplet[\"nonclone\"]][\"S\"]\n",
    "        N_C = embedding_cache[triplet[\"nonclone\"]][\"C\"]\n",
    "\n",
    "        g = torch.sigmoid(alpha)  # g ∈ (0,1)\n",
    "\n",
    "        A = (1 - g) * A_S + g * A_C\n",
    "        P = (1 - g) * P_S + g * P_C\n",
    "        N = (1 - g) * N_S + g * N_C\n",
    "\n",
    "        sim_pos = cosine_sim(A, P)\n",
    "        sim_neg = cosine_sim(A, N)\n",
    "\n",
    "        loss = torch.relu(0.2 + sim_neg - sim_pos)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(triplets)\n",
    "\n",
    "    # ---- Early stopping logic ----\n",
    "    if avg_loss < best_loss - min_delta:\n",
    "        best_loss = avg_loss\n",
    "        best_alpha = alpha.detach().clone()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    g_val = torch.sigmoid(alpha).item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}: loss={avg_loss:.4f}, \"\n",
    "        f\"wS={1-g_val:.3f}, wC={g_val:.3f}\"\n",
    "    )\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\n",
    "            f\"Early stopping at epoch {epoch}. \"\n",
    "            f\"Best loss={best_loss:.4f}\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "# Restore best alpha (important!)\n",
    "if best_alpha is not None:\n",
    "    alpha.data = best_alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d8c9b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
