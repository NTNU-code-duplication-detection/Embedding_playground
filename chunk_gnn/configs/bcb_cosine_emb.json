{
    "_notes": "Experiment: CosineEmbeddingLoss with margin=0.5. Dense gradients for clones (always push toward sim=1.0), margin-gated for non-clones (push below 0.5). Already supported in trainer â€” no code changes needed.",

    "data": {
        "bcb_root": "data/data_source/dataset_bigclonebench",
        "clone_labels": "clone_labels.txt",
        "dataset_files": "dataset_files",
        "skip_functions": ["37044", "4892654", "6966398", "7550876"]
    },
    "embedding": {
        "model_name": "microsoft/unixcoder-base",
        "embedding_dim": 768,
        "max_tokens": 512,
        "batch_size": 32
    },
    "graph": {
        "edge_types": ["sequential", "parent_child"],
        "self_loops": true
    },
    "model": {
        "gnn_hidden_dim": 256,
        "gnn_output_dim": 128,
        "gnn_layers": 2,
        "gnn_type": "GCNConv [INFO]",
        "pooling": "global_mean_pool [INFO]",
        "dropout": 0.1,
        "input_projection": true,
        "residual": false,
        "l2_normalize": true
    },
    "training": {
        "epochs": 30,
        "batch_size": 32,
        "learning_rate": 0.0005,
        "weight_decay": 1e-4,
        "loss": "cosine_embedding",
        "margin": 0.5,
        "label_positive": 1.0,
        "label_negative": -1.0,
        "save_epoch_interval": 1,
        "eval_epoch_interval": 1,
        "log_batch_interval": 100,
        "seed": 42,
        "scheduler": "reduce_on_plateau",
        "scheduler_patience": 5,
        "scheduler_factor": 0.5,
        "scheduler_min_lr": 1e-6,
        "early_stop_patience": 10
    },
    "evaluation": {
        "threshold_sweep": true,
        "threshold_range": [-1.0, 1.0],
        "threshold_steps": 200
    }
}
